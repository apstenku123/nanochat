
                                                       ‚ñà‚ñà‚ñà‚ñà‚ñà                ‚ñà‚ñà‚ñà‚ñà‚ñà
                                                      ‚ñë‚ñë‚ñà‚ñà‚ñà                ‚ñë‚ñë‚ñà‚ñà‚ñà
     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë
     ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñë  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà
     ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà
     ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà
    ‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   ‚ñë‚ñë‚ñë‚ñë‚ñë
    
Autodetected device type: cuda
2026-01-11 17:02:13,722 - nanochat.common - [32m[1mINFO[0m - Distributed world size: 1
HW Capability: NVFP4=True () | FP8=True
Precision: NVFP4 (E2M1) + WGrad BF16
Vocab size: 65,536
num_layers: 20
model_dim: 1280
num_heads: 10
num_kv_heads: 10
Tokens / micro-batch / rank: 32 x 2048 = 65,536
Tokens / micro-batch: 65,536
Total batch size 524,288 => gradient accumulation steps: 8
Converted model to bfloat16 for TE training
Number of parameters: 560,988,160 (scaling: 560,988,160)
Estimated FLOPs per token: 3.491758e+09
Calculated number of iterations from target data:param ratio: 21,400
Total number of training tokens: 11,219,763,200
Tokens : Params ratio: 20.00
Total training FLOPs estimate: 3.917670e+19
Scaling the LR for the AdamW parameters ‚àù1/‚àö(1280/768) = 0.774597
Step 00000 | Validation bpb: 3.3017
step 00000/21400 (0.00%) | loss: 11.090637 | lrm: 1.00 | dt: 33990.08ms | tok/sec: 15,424 | mfu: 10.77 | total time: 0.00m
step 00001/21400 (0.00%) | loss: 10.824647 | lrm: 1.00 | dt: 34064.54ms | tok/sec: 15,391 | mfu: 10.75 | total time: 0.00m
step 00002/21400 (0.01%) | loss: 10.033159 | lrm: 1.00 | dt: 32096.85ms | tok/sec: 16,334 | mfu: 11.41 | total time: 0.00m
step 00003/21400 (0.01%) | loss: 9.320753 | lrm: 1.00 | dt: 31230.53ms | tok/sec: 16,787 | mfu: 11.72 | total time: 0.00m
step 00004/21400 (0.02%) | loss: 8.776032 | lrm: 1.00 | dt: 33739.49ms | tok/sec: 15,539 | mfu: 10.85 | total time: 0.00m
step 00005/21400 (0.02%) | loss: 8.400724 | lrm: 1.00 | dt: 34382.51ms | tok/sec: 15,248 | mfu: 10.65 | total time: 0.00m
step 00006/21400 (0.03%) | loss: 8.159862 | lrm: 1.00 | dt: 33066.91ms | tok/sec: 15,855 | mfu: 11.07 | total time: 0.00m
step 00007/21400 (0.03%) | loss: 7.933413 | lrm: 1.00 | dt: 31441.35ms | tok/sec: 16,675 | mfu: 11.65 | total time: 0.00m
step 00008/21400 (0.04%) | loss: 7.743527 | lrm: 1.00 | dt: 32240.52ms | tok/sec: 16,261 | mfu: 11.36 | total time: 0.00m
step 00009/21400 (0.04%) | loss: 7.566499 | lrm: 1.00 | dt: 34033.17ms | tok/sec: 15,405 | mfu: 10.76 | total time: 0.00m
step 00010/21400 (0.05%) | loss: 7.426396 | lrm: 1.00 | dt: 33711.18ms | tok/sec: 15,552 | mfu: 10.86 | total time: 0.00m
step 00011/21400 (0.05%) | loss: 7.316477 | lrm: 1.00 | dt: 30978.37ms | tok/sec: 16,924 | mfu: 11.82 | total time: 0.52m | eta: 11043.3m
step 00012/21400 (0.06%) | loss: 7.215750 | lrm: 1.00 | dt: 30931.93ms | tok/sec: 16,949 | mfu: 11.84 | total time: 1.03m | eta: 11034.5m
step 00013/21400 (0.06%) | loss: 7.115649 | lrm: 1.00 | dt: 33171.61ms | tok/sec: 15,805 | mfu: 11.04 | total time: 1.58m | eta: 11297.3m
step 00014/21400 (0.07%) | loss: 7.020359 | lrm: 1.00 | dt: 33877.91ms | tok/sec: 15,475 | mfu: 10.81 | total time: 2.15m | eta: 11491.4m
step 00015/21400 (0.07%) | loss: 6.945718 | lrm: 1.00 | dt: 33860.46ms | tok/sec: 15,483 | mfu: 10.81 | total time: 2.71m | eta: 11606.4m
step 00016/21400 (0.07%) | loss: 6.872998 | lrm: 1.00 | dt: 34040.62ms | tok/sec: 15,401 | mfu: 10.76 | total time: 3.28m | eta: 11693.5m
step 00017/21400 (0.08%) | loss: 6.798529 | lrm: 1.00 | dt: 32043.90ms | tok/sec: 16,361 | mfu: 11.43 | total time: 3.82m | eta: 11654.0m
step 00018/21400 (0.08%) | loss: 6.732329 | lrm: 1.00 | dt: 30820.31ms | tok/sec: 17,011 | mfu: 11.88 | total time: 4.33m | eta: 11569.7m
step 00019/21400 (0.09%) | loss: 6.684558 | lrm: 1.00 | dt: 30528.57ms | tok/sec: 17,173 | mfu: 11.99 | total time: 4.84m | eta: 11492.4m
step 00020/21400 (0.09%) | loss: 6.638274 | lrm: 1.00 | dt: 31329.26ms | tok/sec: 16,734 | mfu: 11.69 | total time: 5.36m | eta: 11459.1m
step 00021/21400 (0.10%) | loss: 6.600085 | lrm: 1.00 | dt: 31065.04ms | tok/sec: 16,877 | mfu: 11.79 | total time: 5.88m | eta: 11423.1m
step 00022/21400 (0.10%) | loss: 6.580820 | lrm: 1.00 | dt: 31547.36ms | tok/sec: 16,619 | mfu: 11.61 | total time: 6.40m | eta: 11407.4m
