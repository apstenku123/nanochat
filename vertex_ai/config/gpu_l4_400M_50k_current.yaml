# Vertex AI Custom Job Configuration for GPU L4 Training
# 400M Model, 50,000 iterations with FIM 0.4 and current (PyTorch) kernel
#
# CRITICAL: num_iterations=50000 (FIFTY THOUSAND, NOT 5000!)

workerPoolSpecs:
  - machineSpec:
      machineType: g2-standard-8
      acceleratorType: NVIDIA_L4
      acceleratorCount: 1

    replicaCount: 1

    diskSpec:
      bootDiskType: pd-ssd
      bootDiskSizeGb: 500

    containerSpec:
      imageUri: us-central1-docker.pkg.dev/alpine-aspect-459819-m4/nanochat/gpu-trainer:latest

      command:
        - /bin/bash
        - -c

      args:
        - |
          set -e
          echo "=============================================="
          echo "nanochat 400M Training (L4 GPU - PyTorch Current)"
          echo "=============================================="
          echo "CRITICAL PARAMETERS:"
          echo "  depth=16"
          echo "  num_iterations=50000 (FIFTY THOUSAND)"
          echo "  fim_rate=0.4"
          echo "  kernel=current (plain PyTorch, most compatible)"
          echo "=============================================="
          
          nvidia-smi
          
          mkdir -p /app/data/base_data /app/data/tokenizer
          
          echo "=== Downloading tokenizer ==="
          gsutil -m cp -r gs://nanochat-training-data-2026/tokenizer/* /app/data/tokenizer/
          ls -la /app/data/tokenizer/
          
          echo "=== Downloading training data ==="
          gsutil -m cp gs://nanochat-training-data-2026/parquet/base_data_v3/*.parquet /app/data/base_data/
          echo "Downloaded $(ls /app/data/base_data/*.parquet | wc -l) parquet files"
          
          export NANOCHAT_BASE_DIR=/app/data
          
          # Login to wandb 
          wandb login $WANDB_API_KEY
          
          echo "=== Starting training ==="
          python -m scripts.base_train \
              --depth=16 \
              --num_iterations=50000 \
              --fim_rate=0.4 \
              --kernel=current \
              --run=d16_400M_vertex_50k_l4_current \
              --device_batch_size=16 \
              --total_batch_size=262144 \
              --max_seq_len=2048 \
              --eval_every=1000 \
              --core_metric_every=5000

      env:
        - name: HF_TOKEN
          value: "${HF_TOKEN}"
        - name: WANDB_API_KEY
          value: "${WANDB_API_KEY}"
        - name: GOOGLE_CLOUD_PROJECT
          value: "alpine-aspect-459819-m4"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "expandable_segments:True"
        - name: WANDB_PROJECT
          value: "nanochat"
        - name: NANOCHAT_BASE_DIR
          value: "/app/data"
