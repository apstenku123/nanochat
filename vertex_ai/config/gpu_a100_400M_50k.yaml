# Vertex AI Custom Job Configuration for GPU A100 Training
# 400M Model, 50,000 iterations with FIM 0.4 and CCE kernel
#
# CRITICAL: num_iterations=50000 (FIFTY THOUSAND, NOT 5000!)
#
# Usage: gcloud ai custom-jobs create --region=us-central1 --display-name="nanochat-d16-400M-vertex-50k" --config=vertex_ai/config/gpu_a100_400M_50k.yaml
# Cost estimate: ~$50-100 for A100 40GB at ~$3/hour over 15-30 hours

workerPoolSpecs:
  - machineSpec:
      # A100 40GB - best price/performance for 400M model
      machineType: a2-highgpu-1g
      acceleratorType: NVIDIA_TESLA_A100
      acceleratorCount: 1

    replicaCount: 1

    diskSpec:
      bootDiskType: pd-ssd
      bootDiskSizeGb: 200

    containerSpec:
      # Use our custom GPU trainer image
      imageUri: us-central1-docker.pkg.dev/alpine-aspect-459819-m4/nanochat/gpu-trainer:latest

      # CRITICAL TRAINING ARGUMENTS - DO NOT MODIFY WITHOUT APPROVAL
      args:
        - "--depth=16"                    # 400M model (16 * 64 aspect_ratio = 1024 model_dim)
        - "--num_iterations=50000"        # FIFTY THOUSAND steps - NOT 5000!
        - "--fim_rate=0.4"               # 40% Fill-in-the-Middle
        - "--structured_fim_rate=0.2"    # 20% structured FIM (docstrings/signatures)
        - "--kernel=cce"                  # Apple Cut Cross Entropy
        - "--run=d16_400M_vertex_50k"     # wandb run name
        - "--device_batch_size=32"        # Per-GPU batch size
        - "--total_batch_size=524288"     # 512K tokens per optimization step
        - "--max_seq_len=2048"            # 2K context window
        - "--eval_every=1000"             # Evaluate every 1K steps
        - "--core_metric_every=5000"      # CORE eval every 5K steps
        - "--warmup_ratio=0.0"
        - "--warmdown_ratio=0.4"

      # Environment variables
      env:
        - name: HF_TOKEN
          value: "${HF_TOKEN}"
        - name: WANDB_API_KEY
          value: "${WANDB_API_KEY}"
        - name: GOOGLE_CLOUD_PROJECT
          value: "alpine-aspect-459819-m4"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "expandable_segments:True"
        - name: WANDB_PROJECT
          value: "nanochat"
        - name: NANOCHAT_BASE_DIR
          value: "/app/data"
        - name: NANOCHAT_CPP_TOKENIZER
          value: "1"

# Use standard scheduling (not spot) for reliability on 50K step run
# scheduling:
#   strategy: STANDARD
