Kernel backend: cce
Autodetected device type: cuda
2026-02-02 17:08:00,507 - nanochat.common - [32m[1mINFO[0m - Distributed world size: 1
Vocab size: 32,768
Loading SFT dataset from data/diff_sft.jsonl
SFT dataset size: 60052 examples
Loading pretrained model from /home/dave/.cache/nanochat/base_checkpoints/d16_400M_fim_cce
Loaded checkpoint from step 10000
Parameters: 398,458,912
Training for 2 epochs, 15013 steps/epoch, 30026 total steps
Warmup steps: 3002
  step 00050/30026 | epoch 1/2 | loss: 1.7691 | avg: 1.5291 | lr: 8.33e-07
  step 00100/30026 | epoch 1/2 | loss: 1.0844 | avg: 1.5730 | lr: 1.67e-06
  step 00150/30026 | epoch 1/2 | loss: 1.6931 | avg: 1.5588 | lr: 2.50e-06
  step 00200/30026 | epoch 1/2 | loss: 1.5516 | avg: 1.5696 | lr: 3.33e-06
  step 00250/30026 | epoch 1/2 | loss: 1.6081 | avg: 1.5735 | lr: 4.16e-06
  step 00300/30026 | epoch 1/2 | loss: 1.4367 | avg: 1.5677 | lr: 5.00e-06
  step 00350/30026 | epoch 1/2 | loss: 1.0655 | avg: 1.5625 | lr: 5.83e-06
  step 00400/30026 | epoch 1/2 | loss: 1.7076 | avg: 1.5591 | lr: 6.66e-06
  step 00450/30026 | epoch 1/2 | loss: 1.1997 | avg: 1.5615 | lr: 7.50e-06
  step 00500/30026 | epoch 1/2 | loss: 1.7517 | avg: 1.5575 | lr: 8.33e-06
  step 00550/30026 | epoch 1/2 | loss: 0.8418 | avg: 1.5468 | lr: 9.16e-06
  step 00600/30026 | epoch 1/2 | loss: 1.7541 | avg: 1.5453 | lr: 9.99e-06
  step 00650/30026 | epoch 1/2 | loss: 1.6950 | avg: 1.5351 | lr: 1.08e-05
  step 00700/30026 | epoch 1/2 | loss: 1.4054 | avg: 1.5297 | lr: 1.17e-05
  step 00750/30026 | epoch 1/2 | loss: 1.4454 | avg: 1.5205 | lr: 1.25e-05
  step 00800/30026 | epoch 1/2 | loss: 1.3250 | avg: 1.5089 | lr: 1.33e-05
  step 00850/30026 | epoch 1/2 | loss: 1.6420 | avg: 1.4990 | lr: 1.42e-05
  step 00900/30026 | epoch 1/2 | loss: 0.8546 | avg: 1.4892 | lr: 1.50e-05
  step 00950/30026 | epoch 1/2 | loss: 1.9069 | avg: 1.4817 | lr: 1.58e-05
  step 01000/30026 | epoch 1/2 | loss: 0.8690 | avg: 1.4674 | lr: 1.67e-05
  step 01050/30026 | epoch 1/2 | loss: 0.8910 | avg: 1.4574 | lr: 1.75e-05
  step 01100/30026 | epoch 1/2 | loss: 1.4552 | avg: 1.4462 | lr: 1.83e-05
  step 01150/30026 | epoch 1/2 | loss: 0.9496 | avg: 1.4322 | lr: 1.92e-05
  step 01200/30026 | epoch 1/2 | loss: 1.5569 | avg: 1.4236 | lr: 2.00e-05
  step 01250/30026 | epoch 1/2 | loss: 0.9276 | avg: 1.4126 | lr: 2.08e-05
  step 01300/30026 | epoch 1/2 | loss: 1.0333 | avg: 1.4018 | lr: 2.17e-05
  step 01350/30026 | epoch 1/2 | loss: 1.3282 | avg: 1.3939 | lr: 2.25e-05
  step 01400/30026 | epoch 1/2 | loss: 0.9589 | avg: 1.3859 | lr: 2.33e-05
  step 01450/30026 | epoch 1/2 | loss: 1.1716 | avg: 1.3742 | lr: 2.42e-05
  step 01500/30026 | epoch 1/2 | loss: 1.2771 | avg: 1.3668 | lr: 2.50e-05
  step 01550/30026 | epoch 1/2 | loss: 0.9043 | avg: 1.3584 | lr: 2.58e-05
  step 01600/30026 | epoch 1/2 | loss: 0.9769 | avg: 1.3503 | lr: 2.66e-05
  step 01650/30026 | epoch 1/2 | loss: 1.0695 | avg: 1.3448 | lr: 2.75e-05
  step 01700/30026 | epoch 1/2 | loss: 1.3360 | avg: 1.3382 | lr: 2.83e-05
  step 01750/30026 | epoch 1/2 | loss: 1.7836 | avg: 1.3314 | lr: 2.91e-05
  step 01800/30026 | epoch 1/2 | loss: 1.4243 | avg: 1.3250 | lr: 3.00e-05
  step 01850/30026 | epoch 1/2 | loss: 0.4209 | avg: 1.3208 | lr: 3.08e-05
  step 01900/30026 | epoch 1/2 | loss: 1.0038 | avg: 1.3158 | lr: 3.16e-05
  step 01950/30026 | epoch 1/2 | loss: 0.8218 | avg: 1.3099 | lr: 3.25e-05
  step 02000/30026 | epoch 1/2 | loss: 1.2411 | avg: 1.3049 | lr: 3.33e-05
  step 02050/30026 | epoch 1/2 | loss: 1.6277 | avg: 1.2996 | lr: 3.41e-05
  step 02100/30026 | epoch 1/2 | loss: 1.0776 | avg: 1.2934 | lr: 3.50e-05
  step 02150/30026 | epoch 1/2 | loss: 1.3433 | avg: 1.2885 | lr: 3.58e-05
  step 02200/30026 | epoch 1/2 | loss: 1.0575 | avg: 1.2827 | lr: 3.66e-05
  step 02250/30026 | epoch 1/2 | loss: 1.4803 | avg: 1.2784 | lr: 3.75e-05
  step 02300/30026 | epoch 1/2 | loss: 0.8274 | avg: 1.2735 | lr: 3.83e-05
  step 02350/30026 | epoch 1/2 | loss: 1.0533 | avg: 1.2678 | lr: 3.91e-05
  step 02400/30026 | epoch 1/2 | loss: 1.0178 | avg: 1.2646 | lr: 4.00e-05
  step 02450/30026 | epoch 1/2 | loss: 1.1904 | avg: 1.2597 | lr: 4.08e-05
  step 02500/30026 | epoch 1/2 | loss: 1.3141 | avg: 1.2565 | lr: 4.16e-05
  step 02550/30026 | epoch 1/2 | loss: 0.9229 | avg: 1.2523 | lr: 4.25e-05
  step 02600/30026 | epoch 1/2 | loss: 0.3313 | avg: 1.2494 | lr: 4.33e-05
  step 02650/30026 | epoch 1/2 | loss: 0.9018 | avg: 1.2451 | lr: 4.41e-05
  step 02700/30026 | epoch 1/2 | loss: 1.1920 | avg: 1.2422 | lr: 4.50e-05
  step 02750/30026 | epoch 1/2 | loss: 1.0004 | avg: 1.2381 | lr: 4.58e-05
  step 02800/30026 | epoch 1/2 | loss: 0.8943 | avg: 1.2332 | lr: 4.66e-05
  step 02850/30026 | epoch 1/2 | loss: 1.2138 | avg: 1.2287 | lr: 4.75e-05
  step 02900/30026 | epoch 1/2 | loss: 0.8856 | avg: 1.2257 | lr: 4.83e-05
  step 02950/30026 | epoch 1/2 | loss: 1.0212 | avg: 1.2230 | lr: 4.91e-05
  step 03000/30026 | epoch 1/2 | loss: 1.2177 | avg: 1.2206 | lr: 5.00e-05
  step 03050/30026 | epoch 1/2 | loss: 0.7758 | avg: 1.2178 | lr: 5.00e-05
  step 03100/30026 | epoch 1/2 | loss: 0.8198 | avg: 1.2144 | lr: 5.00e-05
  step 03150/30026 | epoch 1/2 | loss: 0.5207 | avg: 1.2120 | lr: 5.00e-05
  step 03200/30026 | epoch 1/2 | loss: 1.0377 | avg: 1.2083 | lr: 5.00e-05
  step 03250/30026 | epoch 1/2 | loss: 1.4475 | avg: 1.2059 | lr: 5.00e-05
  step 03300/30026 | epoch 1/2 | loss: 1.1319 | avg: 1.2035 | lr: 5.00e-05
  step 03350/30026 | epoch 1/2 | loss: 1.3924 | avg: 1.2004 | lr: 5.00e-05
  step 03400/30026 | epoch 1/2 | loss: 0.6150 | avg: 1.1979 | lr: 5.00e-05
  step 03450/30026 | epoch 1/2 | loss: 1.1825 | avg: 1.1941 | lr: 5.00e-05
  step 03500/30026 | epoch 1/2 | loss: 0.7575 | avg: 1.1916 | lr: 5.00e-05
  step 03550/30026 | epoch 1/2 | loss: 0.9083 | avg: 1.1890 | lr: 4.99e-05
  step 03600/30026 | epoch 1/2 | loss: 0.9620 | avg: 1.1855 | lr: 4.99e-05
  step 03650/30026 | epoch 1/2 | loss: 0.9786 | avg: 1.1832 | lr: 4.99e-05
  step 03700/30026 | epoch 1/2 | loss: 1.0371 | avg: 1.1817 | lr: 4.99e-05
  step 03750/30026 | epoch 1/2 | loss: 0.7670 | avg: 1.1790 | lr: 4.99e-05
  step 03800/30026 | epoch 1/2 | loss: 1.1811 | avg: 1.1767 | lr: 4.99e-05
  step 03850/30026 | epoch 1/2 | loss: 1.1096 | avg: 1.1744 | lr: 4.99e-05
  step 03900/30026 | epoch 1/2 | loss: 0.6034 | avg: 1.1710 | lr: 4.99e-05
  step 03950/30026 | epoch 1/2 | loss: 0.5520 | avg: 1.1683 | lr: 4.98e-05
  step 04000/30026 | epoch 1/2 | loss: 1.0871 | avg: 1.1664 | lr: 4.98e-05
  step 04050/30026 | epoch 1/2 | loss: 0.8479 | avg: 1.1647 | lr: 4.98e-05
  step 04100/30026 | epoch 1/2 | loss: 0.7779 | avg: 1.1624 | lr: 4.98e-05
  step 04150/30026 | epoch 1/2 | loss: 1.1422 | avg: 1.1603 | lr: 4.98e-05
  step 04200/30026 | epoch 1/2 | loss: 1.4584 | avg: 1.1590 | lr: 4.98e-05
  step 04250/30026 | epoch 1/2 | loss: 1.1707 | avg: 1.1572 | lr: 4.97e-05
  step 04300/30026 | epoch 1/2 | loss: 1.2990 | avg: 1.1558 | lr: 4.97e-05
  step 04350/30026 | epoch 1/2 | loss: 1.0676 | avg: 1.1538 | lr: 4.97e-05
  step 04400/30026 | epoch 1/2 | loss: 0.9911 | avg: 1.1523 | lr: 4.97e-05
  step 04450/30026 | epoch 1/2 | loss: 0.5189 | avg: 1.1501 | lr: 4.96e-05
  step 04500/30026 | epoch 1/2 | loss: 0.9568 | avg: 1.1489 | lr: 4.96e-05
  step 04550/30026 | epoch 1/2 | loss: 1.1338 | avg: 1.1472 | lr: 4.96e-05
  step 04600/30026 | epoch 1/2 | loss: 1.0478 | avg: 1.1453 | lr: 4.96e-05
  step 04650/30026 | epoch 1/2 | loss: 0.7672 | avg: 1.1434 | lr: 4.95e-05
  step 04700/30026 | epoch 1/2 | loss: 1.1184 | avg: 1.1414 | lr: 4.95e-05
  step 04750/30026 | epoch 1/2 | loss: 1.4866 | avg: 1.1400 | lr: 4.95e-05
  step 04800/30026 | epoch 1/2 | loss: 1.0363 | avg: 1.1384 | lr: 4.95e-05
  step 04850/30026 | epoch 1/2 | loss: 0.5015 | avg: 1.1365 | lr: 4.94e-05
  step 04900/30026 | epoch 1/2 | loss: 1.2621 | avg: 1.1359 | lr: 4.94e-05
  step 04950/30026 | epoch 1/2 | loss: 0.8928 | avg: 1.1343 | lr: 4.94e-05
  step 05000/30026 | epoch 1/2 | loss: 1.2058 | avg: 1.1329 | lr: 4.93e-05
2026-02-02 17:36:27,238 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_005000.pt
2026-02-02 17:36:27,238 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_005000.json
2026-02-02 17:36:28,389 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_005000_rank0.pt
  step 05050/30026 | epoch 1/2 | loss: 0.5946 | avg: 1.1314 | lr: 4.93e-05
  step 05100/30026 | epoch 1/2 | loss: 1.1767 | avg: 1.1300 | lr: 4.93e-05
  step 05150/30026 | epoch 1/2 | loss: 0.8378 | avg: 1.1286 | lr: 4.92e-05
  step 05200/30026 | epoch 1/2 | loss: 0.9084 | avg: 1.1273 | lr: 4.92e-05
  step 05250/30026 | epoch 1/2 | loss: 0.7503 | avg: 1.1255 | lr: 4.92e-05
  step 05300/30026 | epoch 1/2 | loss: 1.2167 | avg: 1.1243 | lr: 4.91e-05
  step 05350/30026 | epoch 1/2 | loss: 0.8576 | avg: 1.1230 | lr: 4.91e-05
  step 05400/30026 | epoch 1/2 | loss: 0.5598 | avg: 1.1216 | lr: 4.90e-05
  step 05450/30026 | epoch 1/2 | loss: 1.2826 | avg: 1.1206 | lr: 4.90e-05
  step 05500/30026 | epoch 1/2 | loss: 1.1940 | avg: 1.1192 | lr: 4.90e-05
  step 05550/30026 | epoch 1/2 | loss: 1.2519 | avg: 1.1186 | lr: 4.89e-05
  step 05600/30026 | epoch 1/2 | loss: 0.9437 | avg: 1.1176 | lr: 4.89e-05
  step 05650/30026 | epoch 1/2 | loss: 0.9873 | avg: 1.1166 | lr: 4.88e-05
  step 05700/30026 | epoch 1/2 | loss: 1.0377 | avg: 1.1153 | lr: 4.88e-05
  step 05750/30026 | epoch 1/2 | loss: 0.6945 | avg: 1.1143 | lr: 4.87e-05
  step 05800/30026 | epoch 1/2 | loss: 1.0575 | avg: 1.1130 | lr: 4.87e-05
  step 05850/30026 | epoch 1/2 | loss: 1.1589 | avg: 1.1117 | lr: 4.86e-05
  step 05900/30026 | epoch 1/2 | loss: 1.0121 | avg: 1.1107 | lr: 4.86e-05
  step 05950/30026 | epoch 1/2 | loss: 1.4207 | avg: 1.1095 | lr: 4.85e-05
  step 06000/30026 | epoch 1/2 | loss: 0.6354 | avg: 1.1087 | lr: 4.85e-05
  step 06050/30026 | epoch 1/2 | loss: 0.7648 | avg: 1.1075 | lr: 4.84e-05
  step 06100/30026 | epoch 1/2 | loss: 1.1364 | avg: 1.1061 | lr: 4.84e-05
  step 06150/30026 | epoch 1/2 | loss: 0.5908 | avg: 1.1050 | lr: 4.83e-05
  step 06200/30026 | epoch 1/2 | loss: 1.1736 | avg: 1.1042 | lr: 4.83e-05
  step 06250/30026 | epoch 1/2 | loss: 0.2588 | avg: 1.1029 | lr: 4.82e-05
  step 06300/30026 | epoch 1/2 | loss: 0.6923 | avg: 1.1018 | lr: 4.82e-05
  step 06350/30026 | epoch 1/2 | loss: 0.9523 | avg: 1.1013 | lr: 4.81e-05
  step 06400/30026 | epoch 1/2 | loss: 1.3817 | avg: 1.1006 | lr: 4.81e-05
  step 06450/30026 | epoch 1/2 | loss: 0.5718 | avg: 1.0994 | lr: 4.80e-05
  step 06500/30026 | epoch 1/2 | loss: 1.1881 | avg: 1.0987 | lr: 4.80e-05
  step 06550/30026 | epoch 1/2 | loss: 0.9797 | avg: 1.0976 | lr: 4.79e-05
  step 06600/30026 | epoch 1/2 | loss: 0.8839 | avg: 1.0965 | lr: 4.78e-05
  step 06650/30026 | epoch 1/2 | loss: 0.8597 | avg: 1.0958 | lr: 4.78e-05
  step 06700/30026 | epoch 1/2 | loss: 0.7101 | avg: 1.0948 | lr: 4.77e-05
  step 06750/30026 | epoch 1/2 | loss: 1.1940 | avg: 1.0938 | lr: 4.77e-05
  step 06800/30026 | epoch 1/2 | loss: 0.7398 | avg: 1.0924 | lr: 4.76e-05
  step 06850/30026 | epoch 1/2 | loss: 0.8778 | avg: 1.0913 | lr: 4.75e-05
  step 06900/30026 | epoch 1/2 | loss: 0.3834 | avg: 1.0906 | lr: 4.75e-05
  step 06950/30026 | epoch 1/2 | loss: 0.5397 | avg: 1.0893 | lr: 4.74e-05
  step 07000/30026 | epoch 1/2 | loss: 0.7678 | avg: 1.0884 | lr: 4.73e-05
  step 07050/30026 | epoch 1/2 | loss: 0.5182 | avg: 1.0875 | lr: 4.73e-05
  step 07100/30026 | epoch 1/2 | loss: 0.8277 | avg: 1.0868 | lr: 4.72e-05
  step 07150/30026 | epoch 1/2 | loss: 0.9125 | avg: 1.0863 | lr: 4.72e-05
  step 07200/30026 | epoch 1/2 | loss: 1.2416 | avg: 1.0855 | lr: 4.71e-05
  step 07250/30026 | epoch 1/2 | loss: 0.9198 | avg: 1.0848 | lr: 4.70e-05
  step 07300/30026 | epoch 1/2 | loss: 0.7597 | avg: 1.0841 | lr: 4.69e-05
  step 07350/30026 | epoch 1/2 | loss: 1.0632 | avg: 1.0833 | lr: 4.69e-05
  step 07400/30026 | epoch 1/2 | loss: 0.9911 | avg: 1.0827 | lr: 4.68e-05
  step 07450/30026 | epoch 1/2 | loss: 0.7739 | avg: 1.0821 | lr: 4.67e-05
  step 07500/30026 | epoch 1/2 | loss: 1.1261 | avg: 1.0817 | lr: 4.67e-05
  step 07550/30026 | epoch 1/2 | loss: 0.9420 | avg: 1.0811 | lr: 4.66e-05
  step 07600/30026 | epoch 1/2 | loss: 0.8825 | avg: 1.0804 | lr: 4.65e-05
  step 07650/30026 | epoch 1/2 | loss: 1.5070 | avg: 1.0797 | lr: 4.64e-05
  step 07700/30026 | epoch 1/2 | loss: 0.6964 | avg: 1.0791 | lr: 4.64e-05
  step 07750/30026 | epoch 1/2 | loss: 0.9458 | avg: 1.0783 | lr: 4.63e-05
  step 07800/30026 | epoch 1/2 | loss: 0.7307 | avg: 1.0773 | lr: 4.62e-05
  step 07850/30026 | epoch 1/2 | loss: 1.4167 | avg: 1.0768 | lr: 4.61e-05
  step 07900/30026 | epoch 1/2 | loss: 1.1198 | avg: 1.0762 | lr: 4.61e-05
  step 07950/30026 | epoch 1/2 | loss: 0.7513 | avg: 1.0751 | lr: 4.60e-05
  step 08000/30026 | epoch 1/2 | loss: 0.9605 | avg: 1.0743 | lr: 4.59e-05
  step 08050/30026 | epoch 1/2 | loss: 1.2580 | avg: 1.0735 | lr: 4.58e-05
  step 08100/30026 | epoch 1/2 | loss: 1.1826 | avg: 1.0725 | lr: 4.57e-05
  step 08150/30026 | epoch 1/2 | loss: 1.4129 | avg: 1.0718 | lr: 4.57e-05
  step 08200/30026 | epoch 1/2 | loss: 1.1525 | avg: 1.0713 | lr: 4.56e-05
  step 08250/30026 | epoch 1/2 | loss: 0.9766 | avg: 1.0706 | lr: 4.55e-05
  step 08300/30026 | epoch 1/2 | loss: 0.8799 | avg: 1.0695 | lr: 4.54e-05
  step 08350/30026 | epoch 1/2 | loss: 1.6850 | avg: 1.0691 | lr: 4.53e-05
  step 08400/30026 | epoch 1/2 | loss: 0.9197 | avg: 1.0686 | lr: 4.52e-05
  step 08450/30026 | epoch 1/2 | loss: 1.0288 | avg: 1.0683 | lr: 4.52e-05
  step 08500/30026 | epoch 1/2 | loss: 0.7403 | avg: 1.0677 | lr: 4.51e-05
  step 08550/30026 | epoch 1/2 | loss: 1.1279 | avg: 1.0669 | lr: 4.50e-05
  step 08600/30026 | epoch 1/2 | loss: 0.8234 | avg: 1.0663 | lr: 4.49e-05
  step 08650/30026 | epoch 1/2 | loss: 1.0167 | avg: 1.0655 | lr: 4.48e-05
  step 08700/30026 | epoch 1/2 | loss: 1.3162 | avg: 1.0647 | lr: 4.47e-05
  step 08750/30026 | epoch 1/2 | loss: 0.7714 | avg: 1.0640 | lr: 4.46e-05
  step 08800/30026 | epoch 1/2 | loss: 1.2659 | avg: 1.0636 | lr: 4.45e-05
  step 08850/30026 | epoch 1/2 | loss: 0.8833 | avg: 1.0628 | lr: 4.44e-05
  step 08900/30026 | epoch 1/2 | loss: 1.1457 | avg: 1.0622 | lr: 4.44e-05
  step 08950/30026 | epoch 1/2 | loss: 1.0990 | avg: 1.0616 | lr: 4.43e-05
  step 09000/30026 | epoch 1/2 | loss: 1.1030 | avg: 1.0606 | lr: 4.42e-05
  step 09050/30026 | epoch 1/2 | loss: 0.8578 | avg: 1.0598 | lr: 4.41e-05
  step 09100/30026 | epoch 1/2 | loss: 0.5815 | avg: 1.0593 | lr: 4.40e-05
  step 09150/30026 | epoch 1/2 | loss: 0.7932 | avg: 1.0582 | lr: 4.39e-05
  step 09200/30026 | epoch 1/2 | loss: 0.5714 | avg: 1.0578 | lr: 4.38e-05
  step 09250/30026 | epoch 1/2 | loss: 0.6237 | avg: 1.0568 | lr: 4.37e-05
  step 09300/30026 | epoch 1/2 | loss: 1.0190 | avg: 1.0566 | lr: 4.36e-05
  step 09350/30026 | epoch 1/2 | loss: 0.9396 | avg: 1.0560 | lr: 4.35e-05
  step 09400/30026 | epoch 1/2 | loss: 0.9215 | avg: 1.0554 | lr: 4.34e-05
  step 09450/30026 | epoch 1/2 | loss: 1.1936 | avg: 1.0548 | lr: 4.33e-05
  step 09500/30026 | epoch 1/2 | loss: 1.5265 | avg: 1.0542 | lr: 4.32e-05
  step 09550/30026 | epoch 1/2 | loss: 0.2955 | avg: 1.0536 | lr: 4.31e-05
  step 09600/30026 | epoch 1/2 | loss: 0.9842 | avg: 1.0529 | lr: 4.30e-05
  step 09650/30026 | epoch 1/2 | loss: 1.0708 | avg: 1.0525 | lr: 4.29e-05
  step 09700/30026 | epoch 1/2 | loss: 0.8273 | avg: 1.0520 | lr: 4.28e-05
  step 09750/30026 | epoch 1/2 | loss: 0.8266 | avg: 1.0513 | lr: 4.27e-05
  step 09800/30026 | epoch 1/2 | loss: 1.3004 | avg: 1.0510 | lr: 4.26e-05
  step 09850/30026 | epoch 1/2 | loss: 0.8897 | avg: 1.0502 | lr: 4.25e-05
  step 09900/30026 | epoch 1/2 | loss: 0.9991 | avg: 1.0499 | lr: 4.24e-05
  step 09950/30026 | epoch 1/2 | loss: 0.7232 | avg: 1.0492 | lr: 4.23e-05
  step 10000/30026 | epoch 1/2 | loss: 0.9728 | avg: 1.0489 | lr: 4.22e-05
2026-02-02 18:06:08,498 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_010000.pt
2026-02-02 18:06:08,498 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_010000.json
2026-02-02 18:06:09,454 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_010000_rank0.pt
  step 10050/30026 | epoch 1/2 | loss: 0.5825 | avg: 1.0483 | lr: 4.21e-05
  step 10100/30026 | epoch 1/2 | loss: 0.8913 | avg: 1.0479 | lr: 4.20e-05
  step 10150/30026 | epoch 1/2 | loss: 0.7102 | avg: 1.0478 | lr: 4.19e-05
  step 10200/30026 | epoch 1/2 | loss: 0.9510 | avg: 1.0472 | lr: 4.17e-05
  step 10250/30026 | epoch 1/2 | loss: 0.6256 | avg: 1.0466 | lr: 4.16e-05
  step 10300/30026 | epoch 1/2 | loss: 0.6023 | avg: 1.0458 | lr: 4.15e-05
  step 10350/30026 | epoch 1/2 | loss: 1.0932 | avg: 1.0453 | lr: 4.14e-05
  step 10400/30026 | epoch 1/2 | loss: 1.5572 | avg: 1.0447 | lr: 4.13e-05
  step 10450/30026 | epoch 1/2 | loss: 1.1021 | avg: 1.0445 | lr: 4.12e-05
  step 10500/30026 | epoch 1/2 | loss: 0.6441 | avg: 1.0442 | lr: 4.11e-05
  step 10550/30026 | epoch 1/2 | loss: 0.4647 | avg: 1.0439 | lr: 4.10e-05
  step 10600/30026 | epoch 1/2 | loss: 0.7884 | avg: 1.0435 | lr: 4.09e-05
  step 10650/30026 | epoch 1/2 | loss: 1.0723 | avg: 1.0431 | lr: 4.08e-05
  step 10700/30026 | epoch 1/2 | loss: 0.8562 | avg: 1.0427 | lr: 4.06e-05
  step 10750/30026 | epoch 1/2 | loss: 1.1642 | avg: 1.0422 | lr: 4.05e-05
  step 10800/30026 | epoch 1/2 | loss: 0.4713 | avg: 1.0416 | lr: 4.04e-05
  step 10850/30026 | epoch 1/2 | loss: 1.0712 | avg: 1.0414 | lr: 4.03e-05
  step 10900/30026 | epoch 1/2 | loss: 0.9586 | avg: 1.0410 | lr: 4.02e-05
  step 10950/30026 | epoch 1/2 | loss: 0.2681 | avg: 1.0405 | lr: 4.01e-05
  step 11000/30026 | epoch 1/2 | loss: 0.8998 | avg: 1.0399 | lr: 4.00e-05
  step 11050/30026 | epoch 1/2 | loss: 1.0256 | avg: 1.0395 | lr: 3.98e-05
  step 11100/30026 | epoch 1/2 | loss: 1.1048 | avg: 1.0392 | lr: 3.97e-05
  step 11150/30026 | epoch 1/2 | loss: 0.3739 | avg: 1.0390 | lr: 3.96e-05
  step 11200/30026 | epoch 1/2 | loss: 0.8855 | avg: 1.0386 | lr: 3.95e-05
  step 11250/30026 | epoch 1/2 | loss: 0.6614 | avg: 1.0384 | lr: 3.94e-05
  step 11300/30026 | epoch 1/2 | loss: 0.8332 | avg: 1.0381 | lr: 3.92e-05
  step 11350/30026 | epoch 1/2 | loss: 0.8489 | avg: 1.0378 | lr: 3.91e-05
  step 11400/30026 | epoch 1/2 | loss: 1.2214 | avg: 1.0376 | lr: 3.90e-05
  step 11450/30026 | epoch 1/2 | loss: 0.3903 | avg: 1.0372 | lr: 3.89e-05
  step 11500/30026 | epoch 1/2 | loss: 0.8770 | avg: 1.0366 | lr: 3.88e-05
  step 11550/30026 | epoch 1/2 | loss: 1.1788 | avg: 1.0362 | lr: 3.86e-05
  step 11600/30026 | epoch 1/2 | loss: 1.0453 | avg: 1.0357 | lr: 3.85e-05
  step 11650/30026 | epoch 1/2 | loss: 0.9630 | avg: 1.0353 | lr: 3.84e-05
  step 11700/30026 | epoch 1/2 | loss: 0.9288 | avg: 1.0350 | lr: 3.83e-05
  step 11750/30026 | epoch 1/2 | loss: 0.9159 | avg: 1.0344 | lr: 3.82e-05
  step 11800/30026 | epoch 1/2 | loss: 0.5324 | avg: 1.0341 | lr: 3.80e-05
  step 11850/30026 | epoch 1/2 | loss: 0.7792 | avg: 1.0333 | lr: 3.79e-05
  step 11900/30026 | epoch 1/2 | loss: 0.9395 | avg: 1.0331 | lr: 3.78e-05
  step 11950/30026 | epoch 1/2 | loss: 0.7473 | avg: 1.0329 | lr: 3.77e-05
  step 12000/30026 | epoch 1/2 | loss: 0.5918 | avg: 1.0324 | lr: 3.75e-05
  step 12050/30026 | epoch 1/2 | loss: 1.1553 | avg: 1.0319 | lr: 3.74e-05
  step 12100/30026 | epoch 1/2 | loss: 0.5791 | avg: 1.0318 | lr: 3.73e-05
  step 12150/30026 | epoch 1/2 | loss: 0.3536 | avg: 1.0312 | lr: 3.71e-05
  step 12200/30026 | epoch 1/2 | loss: 0.6998 | avg: 1.0309 | lr: 3.70e-05
  step 12250/30026 | epoch 1/2 | loss: 1.0056 | avg: 1.0307 | lr: 3.69e-05
  step 12300/30026 | epoch 1/2 | loss: 1.0311 | avg: 1.0303 | lr: 3.68e-05
  step 12350/30026 | epoch 1/2 | loss: 0.9834 | avg: 1.0301 | lr: 3.66e-05
  step 12400/30026 | epoch 1/2 | loss: 0.6319 | avg: 1.0295 | lr: 3.65e-05
  step 12450/30026 | epoch 1/2 | loss: 0.7257 | avg: 1.0292 | lr: 3.64e-05
  step 12500/30026 | epoch 1/2 | loss: 1.0992 | avg: 1.0289 | lr: 3.62e-05
  step 12550/30026 | epoch 1/2 | loss: 0.9668 | avg: 1.0286 | lr: 3.61e-05
  step 12600/30026 | epoch 1/2 | loss: 1.1378 | avg: 1.0282 | lr: 3.60e-05
  step 12650/30026 | epoch 1/2 | loss: 1.2169 | avg: 1.0278 | lr: 3.59e-05
  step 12700/30026 | epoch 1/2 | loss: 0.9013 | avg: 1.0275 | lr: 3.57e-05
  step 12750/30026 | epoch 1/2 | loss: 1.0832 | avg: 1.0272 | lr: 3.56e-05
  step 12800/30026 | epoch 1/2 | loss: 0.9762 | avg: 1.0269 | lr: 3.55e-05
  step 12850/30026 | epoch 1/2 | loss: 1.2698 | avg: 1.0265 | lr: 3.53e-05
  step 12900/30026 | epoch 1/2 | loss: 0.9745 | avg: 1.0261 | lr: 3.52e-05
  step 12950/30026 | epoch 1/2 | loss: 1.0558 | avg: 1.0257 | lr: 3.51e-05
  step 13000/30026 | epoch 1/2 | loss: 0.7818 | avg: 1.0251 | lr: 3.49e-05
  step 13050/30026 | epoch 1/2 | loss: 0.8974 | avg: 1.0249 | lr: 3.48e-05
  step 13100/30026 | epoch 1/2 | loss: 0.9043 | avg: 1.0243 | lr: 3.47e-05
  step 13150/30026 | epoch 1/2 | loss: 0.8013 | avg: 1.0239 | lr: 3.45e-05
  step 13200/30026 | epoch 1/2 | loss: 0.9810 | avg: 1.0235 | lr: 3.44e-05
  step 13250/30026 | epoch 1/2 | loss: 0.8532 | avg: 1.0232 | lr: 3.43e-05
  step 13300/30026 | epoch 1/2 | loss: 0.7350 | avg: 1.0228 | lr: 3.41e-05
  step 13350/30026 | epoch 1/2 | loss: 0.8158 | avg: 1.0222 | lr: 3.40e-05
  step 13400/30026 | epoch 1/2 | loss: 0.4542 | avg: 1.0218 | lr: 3.39e-05
  step 13450/30026 | epoch 1/2 | loss: 0.5287 | avg: 1.0214 | lr: 3.37e-05
  step 13500/30026 | epoch 1/2 | loss: 0.8101 | avg: 1.0211 | lr: 3.36e-05
  step 13550/30026 | epoch 1/2 | loss: 0.8515 | avg: 1.0206 | lr: 3.34e-05
  step 13600/30026 | epoch 1/2 | loss: 0.8682 | avg: 1.0203 | lr: 3.33e-05
  step 13650/30026 | epoch 1/2 | loss: 0.3657 | avg: 1.0200 | lr: 3.32e-05
  step 13700/30026 | epoch 1/2 | loss: 0.7403 | avg: 1.0196 | lr: 3.30e-05
  step 13750/30026 | epoch 1/2 | loss: 0.5948 | avg: 1.0193 | lr: 3.29e-05
  step 13800/30026 | epoch 1/2 | loss: 1.2503 | avg: 1.0191 | lr: 3.28e-05
  step 13850/30026 | epoch 1/2 | loss: 1.2375 | avg: 1.0186 | lr: 3.26e-05
  step 13900/30026 | epoch 1/2 | loss: 0.9694 | avg: 1.0183 | lr: 3.25e-05
  step 13950/30026 | epoch 1/2 | loss: 0.7865 | avg: 1.0178 | lr: 3.23e-05
  step 14000/30026 | epoch 1/2 | loss: 1.1184 | avg: 1.0176 | lr: 3.22e-05
  step 14050/30026 | epoch 1/2 | loss: 0.7483 | avg: 1.0173 | lr: 3.21e-05
  step 14100/30026 | epoch 1/2 | loss: 1.0169 | avg: 1.0170 | lr: 3.19e-05
  step 14150/30026 | epoch 1/2 | loss: 0.6112 | avg: 1.0165 | lr: 3.18e-05
  step 14200/30026 | epoch 1/2 | loss: 0.5754 | avg: 1.0165 | lr: 3.16e-05
  step 14250/30026 | epoch 1/2 | loss: 0.9515 | avg: 1.0164 | lr: 3.15e-05
  step 14300/30026 | epoch 1/2 | loss: 0.6934 | avg: 1.0159 | lr: 3.14e-05
  step 14350/30026 | epoch 1/2 | loss: 1.2589 | avg: 1.0157 | lr: 3.12e-05
  step 14400/30026 | epoch 1/2 | loss: 0.9132 | avg: 1.0156 | lr: 3.11e-05
  step 14450/30026 | epoch 1/2 | loss: 0.8680 | avg: 1.0154 | lr: 3.09e-05
  step 14500/30026 | epoch 1/2 | loss: 0.9392 | avg: 1.0150 | lr: 3.08e-05
  step 14550/30026 | epoch 1/2 | loss: 0.5887 | avg: 1.0148 | lr: 3.07e-05
  step 14600/30026 | epoch 1/2 | loss: 0.6295 | avg: 1.0143 | lr: 3.05e-05
  step 14650/30026 | epoch 1/2 | loss: 0.8952 | avg: 1.0139 | lr: 3.04e-05
  step 14700/30026 | epoch 1/2 | loss: 0.9184 | avg: 1.0137 | lr: 3.02e-05
  step 14750/30026 | epoch 1/2 | loss: 0.7060 | avg: 1.0136 | lr: 3.01e-05
  step 14800/30026 | epoch 1/2 | loss: 1.1689 | avg: 1.0134 | lr: 3.00e-05
  step 14850/30026 | epoch 1/2 | loss: 1.2485 | avg: 1.0132 | lr: 2.98e-05
  step 14900/30026 | epoch 1/2 | loss: 0.8687 | avg: 1.0129 | lr: 2.97e-05
  step 14950/30026 | epoch 1/2 | loss: 1.0848 | avg: 1.0127 | lr: 2.95e-05
  step 15000/30026 | epoch 1/2 | loss: 1.1684 | avg: 1.0127 | lr: 2.94e-05
2026-02-02 18:41:00,925 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_015000.pt
2026-02-02 18:41:00,934 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_015000.json
2026-02-02 18:41:02,032 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_015000_rank0.pt
Epoch 1/2 done in 5553.0s | avg loss: 1.0126 | tokens: 29,373,984
  step 15050/30026 | epoch 2/2 | loss: 1.0755 | avg: 0.9756 | lr: 2.92e-05
  step 15100/30026 | epoch 2/2 | loss: 1.3945 | avg: 0.9573 | lr: 2.91e-05
  step 15150/30026 | epoch 2/2 | loss: 0.8882 | avg: 0.9306 | lr: 2.90e-05
  step 15200/30026 | epoch 2/2 | loss: 1.1456 | avg: 0.9164 | lr: 2.88e-05
  step 15250/30026 | epoch 2/2 | loss: 0.9543 | avg: 0.9140 | lr: 2.87e-05
  step 15300/30026 | epoch 2/2 | loss: 1.0112 | avg: 0.9086 | lr: 2.85e-05
  step 15350/30026 | epoch 2/2 | loss: 1.2771 | avg: 0.9105 | lr: 2.84e-05
  step 15400/30026 | epoch 2/2 | loss: 1.0036 | avg: 0.9186 | lr: 2.82e-05
  step 15450/30026 | epoch 2/2 | loss: 1.0530 | avg: 0.9222 | lr: 2.81e-05
  step 15500/30026 | epoch 2/2 | loss: 1.0496 | avg: 0.9209 | lr: 2.79e-05
  step 15550/30026 | epoch 2/2 | loss: 0.8719 | avg: 0.9241 | lr: 2.78e-05
  step 15600/30026 | epoch 2/2 | loss: 1.1228 | avg: 0.9267 | lr: 2.77e-05
  step 15650/30026 | epoch 2/2 | loss: 0.9716 | avg: 0.9251 | lr: 2.75e-05
  step 15700/30026 | epoch 2/2 | loss: 0.8089 | avg: 0.9272 | lr: 2.74e-05
  step 15750/30026 | epoch 2/2 | loss: 0.9363 | avg: 0.9238 | lr: 2.72e-05
  step 15800/30026 | epoch 2/2 | loss: 1.1086 | avg: 0.9235 | lr: 2.71e-05
  step 15850/30026 | epoch 2/2 | loss: 1.0394 | avg: 0.9222 | lr: 2.69e-05
  step 15900/30026 | epoch 2/2 | loss: 0.6814 | avg: 0.9215 | lr: 2.68e-05
  step 15950/30026 | epoch 2/2 | loss: 0.9590 | avg: 0.9223 | lr: 2.66e-05
  step 16000/30026 | epoch 2/2 | loss: 0.7752 | avg: 0.9213 | lr: 2.65e-05
  step 16050/30026 | epoch 2/2 | loss: 0.9812 | avg: 0.9211 | lr: 2.64e-05
  step 16100/30026 | epoch 2/2 | loss: 1.2771 | avg: 0.9193 | lr: 2.62e-05
  step 16150/30026 | epoch 2/2 | loss: 0.6533 | avg: 0.9181 | lr: 2.61e-05
  step 16200/30026 | epoch 2/2 | loss: 1.1122 | avg: 0.9175 | lr: 2.59e-05
  step 16250/30026 | epoch 2/2 | loss: 0.9316 | avg: 0.9172 | lr: 2.58e-05
  step 16300/30026 | epoch 2/2 | loss: 0.9353 | avg: 0.9200 | lr: 2.56e-05
  step 16350/30026 | epoch 2/2 | loss: 0.8753 | avg: 0.9198 | lr: 2.55e-05
  step 16400/30026 | epoch 2/2 | loss: 1.2651 | avg: 0.9189 | lr: 2.53e-05
  step 16450/30026 | epoch 2/2 | loss: 0.9367 | avg: 0.9188 | lr: 2.52e-05
  step 16500/30026 | epoch 2/2 | loss: 0.8242 | avg: 0.9194 | lr: 2.50e-05
  step 16550/30026 | epoch 2/2 | loss: 1.0455 | avg: 0.9187 | lr: 2.49e-05
  step 16600/30026 | epoch 2/2 | loss: 0.6905 | avg: 0.9203 | lr: 2.48e-05
  step 16650/30026 | epoch 2/2 | loss: 0.9055 | avg: 0.9200 | lr: 2.46e-05
  step 16700/30026 | epoch 2/2 | loss: 0.8161 | avg: 0.9193 | lr: 2.45e-05
  step 16750/30026 | epoch 2/2 | loss: 0.2619 | avg: 0.9202 | lr: 2.43e-05
  step 16800/30026 | epoch 2/2 | loss: 1.1334 | avg: 0.9194 | lr: 2.42e-05
  step 16850/30026 | epoch 2/2 | loss: 0.9970 | avg: 0.9200 | lr: 2.40e-05
  step 16900/30026 | epoch 2/2 | loss: 1.1192 | avg: 0.9197 | lr: 2.39e-05
  step 16950/30026 | epoch 2/2 | loss: 1.1566 | avg: 0.9197 | lr: 2.37e-05
  step 17000/30026 | epoch 2/2 | loss: 0.8999 | avg: 0.9216 | lr: 2.36e-05
  step 17050/30026 | epoch 2/2 | loss: 1.0846 | avg: 0.9217 | lr: 2.34e-05
  step 17100/30026 | epoch 2/2 | loss: 1.0549 | avg: 0.9235 | lr: 2.33e-05
  step 17150/30026 | epoch 2/2 | loss: 0.6415 | avg: 0.9229 | lr: 2.32e-05
  step 17200/30026 | epoch 2/2 | loss: 0.7493 | avg: 0.9241 | lr: 2.30e-05
  step 17250/30026 | epoch 2/2 | loss: 0.9808 | avg: 0.9245 | lr: 2.29e-05
  step 17300/30026 | epoch 2/2 | loss: 0.9066 | avg: 0.9252 | lr: 2.27e-05
  step 17350/30026 | epoch 2/2 | loss: 1.1702 | avg: 0.9251 | lr: 2.26e-05
  step 17400/30026 | epoch 2/2 | loss: 0.9728 | avg: 0.9250 | lr: 2.24e-05
  step 17450/30026 | epoch 2/2 | loss: 0.5313 | avg: 0.9257 | lr: 2.23e-05
  step 17500/30026 | epoch 2/2 | loss: 0.7355 | avg: 0.9250 | lr: 2.21e-05
  step 17550/30026 | epoch 2/2 | loss: 0.6603 | avg: 0.9247 | lr: 2.20e-05
  step 17600/30026 | epoch 2/2 | loss: 0.7682 | avg: 0.9254 | lr: 2.19e-05
  step 17650/30026 | epoch 2/2 | loss: 1.3500 | avg: 0.9253 | lr: 2.17e-05
  step 17700/30026 | epoch 2/2 | loss: 0.9294 | avg: 0.9268 | lr: 2.16e-05
  step 17750/30026 | epoch 2/2 | loss: 1.2130 | avg: 0.9265 | lr: 2.14e-05
  step 17800/30026 | epoch 2/2 | loss: 1.0223 | avg: 0.9260 | lr: 2.13e-05
  step 17850/30026 | epoch 2/2 | loss: 1.0304 | avg: 0.9263 | lr: 2.11e-05
  step 17900/30026 | epoch 2/2 | loss: 0.9800 | avg: 0.9271 | lr: 2.10e-05
  step 17950/30026 | epoch 2/2 | loss: 1.1669 | avg: 0.9266 | lr: 2.08e-05
  step 18000/30026 | epoch 2/2 | loss: 0.6727 | avg: 0.9265 | lr: 2.07e-05
  step 18050/30026 | epoch 2/2 | loss: 0.3001 | avg: 0.9279 | lr: 2.06e-05
  step 18100/30026 | epoch 2/2 | loss: 0.8746 | avg: 0.9289 | lr: 2.04e-05
  step 18150/30026 | epoch 2/2 | loss: 0.7430 | avg: 0.9284 | lr: 2.03e-05
  step 18200/30026 | epoch 2/2 | loss: 0.6498 | avg: 0.9270 | lr: 2.01e-05
  step 18250/30026 | epoch 2/2 | loss: 1.2872 | avg: 0.9264 | lr: 2.00e-05
  step 18300/30026 | epoch 2/2 | loss: 1.1111 | avg: 0.9270 | lr: 1.98e-05
  step 18350/30026 | epoch 2/2 | loss: 0.5310 | avg: 0.9275 | lr: 1.97e-05
  step 18400/30026 | epoch 2/2 | loss: 1.1349 | avg: 0.9280 | lr: 1.96e-05
  step 18450/30026 | epoch 2/2 | loss: 1.1351 | avg: 0.9283 | lr: 1.94e-05
  step 18500/30026 | epoch 2/2 | loss: 0.5586 | avg: 0.9282 | lr: 1.93e-05
  step 18550/30026 | epoch 2/2 | loss: 0.8883 | avg: 0.9289 | lr: 1.91e-05
  step 18600/30026 | epoch 2/2 | loss: 0.8473 | avg: 0.9289 | lr: 1.90e-05
  step 18650/30026 | epoch 2/2 | loss: 0.5726 | avg: 0.9294 | lr: 1.89e-05
  step 18700/30026 | epoch 2/2 | loss: 1.2010 | avg: 0.9294 | lr: 1.87e-05
  step 18750/30026 | epoch 2/2 | loss: 0.9093 | avg: 0.9295 | lr: 1.86e-05
  step 18800/30026 | epoch 2/2 | loss: 0.7874 | avg: 0.9293 | lr: 1.84e-05
  step 18850/30026 | epoch 2/2 | loss: 0.4622 | avg: 0.9297 | lr: 1.83e-05
  step 18900/30026 | epoch 2/2 | loss: 1.1660 | avg: 0.9296 | lr: 1.82e-05
  step 18950/30026 | epoch 2/2 | loss: 0.8826 | avg: 0.9300 | lr: 1.80e-05
  step 19000/30026 | epoch 2/2 | loss: 1.0695 | avg: 0.9295 | lr: 1.79e-05
  step 19050/30026 | epoch 2/2 | loss: 0.5837 | avg: 0.9289 | lr: 1.77e-05
  step 19100/30026 | epoch 2/2 | loss: 1.0925 | avg: 0.9288 | lr: 1.76e-05
  step 19150/30026 | epoch 2/2 | loss: 1.3042 | avg: 0.9286 | lr: 1.75e-05
  step 19200/30026 | epoch 2/2 | loss: 1.4511 | avg: 0.9291 | lr: 1.73e-05
  step 19250/30026 | epoch 2/2 | loss: 1.0585 | avg: 0.9296 | lr: 1.72e-05
  step 19300/30026 | epoch 2/2 | loss: 0.6747 | avg: 0.9293 | lr: 1.70e-05
  step 19350/30026 | epoch 2/2 | loss: 0.9426 | avg: 0.9291 | lr: 1.69e-05
  step 19400/30026 | epoch 2/2 | loss: 1.1271 | avg: 0.9285 | lr: 1.68e-05
  step 19450/30026 | epoch 2/2 | loss: 0.7401 | avg: 0.9278 | lr: 1.66e-05
  step 19500/30026 | epoch 2/2 | loss: 0.3901 | avg: 0.9275 | lr: 1.65e-05
  step 19550/30026 | epoch 2/2 | loss: 0.5307 | avg: 0.9277 | lr: 1.64e-05
  step 19600/30026 | epoch 2/2 | loss: 1.1056 | avg: 0.9278 | lr: 1.62e-05
  step 19650/30026 | epoch 2/2 | loss: 0.7855 | avg: 0.9277 | lr: 1.61e-05
  step 19700/30026 | epoch 2/2 | loss: 0.8049 | avg: 0.9278 | lr: 1.60e-05
  step 19750/30026 | epoch 2/2 | loss: 0.7488 | avg: 0.9276 | lr: 1.58e-05
  step 19800/30026 | epoch 2/2 | loss: 0.9459 | avg: 0.9270 | lr: 1.57e-05
  step 19850/30026 | epoch 2/2 | loss: 0.7379 | avg: 0.9272 | lr: 1.55e-05
  step 19900/30026 | epoch 2/2 | loss: 0.8061 | avg: 0.9271 | lr: 1.54e-05
  step 19950/30026 | epoch 2/2 | loss: 0.7891 | avg: 0.9266 | lr: 1.53e-05
  step 20000/30026 | epoch 2/2 | loss: 0.8613 | avg: 0.9262 | lr: 1.51e-05
2026-02-02 19:37:29,179 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_020000.pt
2026-02-02 19:37:29,179 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_020000.json
2026-02-02 19:37:31,383 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_020000_rank0.pt
  step 20050/30026 | epoch 2/2 | loss: 0.9455 | avg: 0.9259 | lr: 1.50e-05
  step 20100/30026 | epoch 2/2 | loss: 0.9844 | avg: 0.9261 | lr: 1.49e-05
  step 20150/30026 | epoch 2/2 | loss: 0.8537 | avg: 0.9263 | lr: 1.47e-05
  step 20200/30026 | epoch 2/2 | loss: 1.4513 | avg: 0.9265 | lr: 1.46e-05
  step 20250/30026 | epoch 2/2 | loss: 1.7349 | avg: 0.9261 | lr: 1.45e-05
  step 20300/30026 | epoch 2/2 | loss: 0.9847 | avg: 0.9260 | lr: 1.44e-05
  step 20350/30026 | epoch 2/2 | loss: 1.0064 | avg: 0.9259 | lr: 1.42e-05
  step 20400/30026 | epoch 2/2 | loss: 1.1199 | avg: 0.9261 | lr: 1.41e-05
  step 20450/30026 | epoch 2/2 | loss: 0.9313 | avg: 0.9263 | lr: 1.40e-05
  step 20500/30026 | epoch 2/2 | loss: 0.8308 | avg: 0.9265 | lr: 1.38e-05
  step 20550/30026 | epoch 2/2 | loss: 0.8719 | avg: 0.9262 | lr: 1.37e-05
  step 20600/30026 | epoch 2/2 | loss: 0.8541 | avg: 0.9266 | lr: 1.36e-05
  step 20650/30026 | epoch 2/2 | loss: 0.9897 | avg: 0.9267 | lr: 1.34e-05
  step 20700/30026 | epoch 2/2 | loss: 0.9812 | avg: 0.9267 | lr: 1.33e-05
  step 20750/30026 | epoch 2/2 | loss: 1.0777 | avg: 0.9269 | lr: 1.32e-05
  step 20800/30026 | epoch 2/2 | loss: 0.9166 | avg: 0.9271 | lr: 1.31e-05
  step 20850/30026 | epoch 2/2 | loss: 0.7337 | avg: 0.9273 | lr: 1.29e-05
  step 20900/30026 | epoch 2/2 | loss: 0.7884 | avg: 0.9273 | lr: 1.28e-05
  step 20950/30026 | epoch 2/2 | loss: 1.1417 | avg: 0.9274 | lr: 1.27e-05
  step 21000/30026 | epoch 2/2 | loss: 1.0480 | avg: 0.9273 | lr: 1.25e-05
  step 21050/30026 | epoch 2/2 | loss: 0.6520 | avg: 0.9268 | lr: 1.24e-05
  step 21100/30026 | epoch 2/2 | loss: 1.0494 | avg: 0.9271 | lr: 1.23e-05
  step 21150/30026 | epoch 2/2 | loss: 1.1443 | avg: 0.9269 | lr: 1.22e-05
  step 21200/30026 | epoch 2/2 | loss: 0.8327 | avg: 0.9268 | lr: 1.20e-05
  step 21250/30026 | epoch 2/2 | loss: 0.4440 | avg: 0.9267 | lr: 1.19e-05
  step 21300/30026 | epoch 2/2 | loss: 1.5161 | avg: 0.9265 | lr: 1.18e-05
  step 21350/30026 | epoch 2/2 | loss: 1.1109 | avg: 0.9270 | lr: 1.17e-05
  step 21400/30026 | epoch 2/2 | loss: 0.8274 | avg: 0.9270 | lr: 1.16e-05
  step 21450/30026 | epoch 2/2 | loss: 1.1698 | avg: 0.9270 | lr: 1.14e-05
  step 21500/30026 | epoch 2/2 | loss: 0.6563 | avg: 0.9267 | lr: 1.13e-05
  step 21550/30026 | epoch 2/2 | loss: 0.9296 | avg: 0.9263 | lr: 1.12e-05
  step 21600/30026 | epoch 2/2 | loss: 0.7431 | avg: 0.9263 | lr: 1.11e-05
  step 21650/30026 | epoch 2/2 | loss: 1.1811 | avg: 0.9266 | lr: 1.09e-05
  step 21700/30026 | epoch 2/2 | loss: 0.9737 | avg: 0.9269 | lr: 1.08e-05
  step 21750/30026 | epoch 2/2 | loss: 0.7069 | avg: 0.9262 | lr: 1.07e-05
  step 21800/30026 | epoch 2/2 | loss: 1.0473 | avg: 0.9264 | lr: 1.06e-05
  step 21850/30026 | epoch 2/2 | loss: 0.9773 | avg: 0.9264 | lr: 1.05e-05
  step 21900/30026 | epoch 2/2 | loss: 1.0205 | avg: 0.9263 | lr: 1.04e-05
  step 21950/30026 | epoch 2/2 | loss: 1.1391 | avg: 0.9261 | lr: 1.02e-05
  step 22000/30026 | epoch 2/2 | loss: 0.8581 | avg: 0.9263 | lr: 1.01e-05
  step 22050/30026 | epoch 2/2 | loss: 0.9772 | avg: 0.9267 | lr: 1.00e-05
  step 22100/30026 | epoch 2/2 | loss: 1.1907 | avg: 0.9264 | lr: 9.88e-06
  step 22150/30026 | epoch 2/2 | loss: 1.0421 | avg: 0.9261 | lr: 9.77e-06
  step 22200/30026 | epoch 2/2 | loss: 0.5839 | avg: 0.9260 | lr: 9.65e-06
  step 22250/30026 | epoch 2/2 | loss: 1.6417 | avg: 0.9261 | lr: 9.54e-06
  step 22300/30026 | epoch 2/2 | loss: 0.7068 | avg: 0.9258 | lr: 9.43e-06
  step 22350/30026 | epoch 2/2 | loss: 1.0117 | avg: 0.9260 | lr: 9.31e-06
  step 22400/30026 | epoch 2/2 | loss: 1.0261 | avg: 0.9261 | lr: 9.20e-06
  step 22450/30026 | epoch 2/2 | loss: 0.8474 | avg: 0.9261 | lr: 9.09e-06
  step 22500/30026 | epoch 2/2 | loss: 0.7354 | avg: 0.9263 | lr: 8.98e-06
  step 22550/30026 | epoch 2/2 | loss: 0.7007 | avg: 0.9264 | lr: 8.86e-06
  step 22600/30026 | epoch 2/2 | loss: 0.9831 | avg: 0.9264 | lr: 8.75e-06
  step 22650/30026 | epoch 2/2 | loss: 0.6071 | avg: 0.9261 | lr: 8.64e-06
  step 22700/30026 | epoch 2/2 | loss: 1.8545 | avg: 0.9261 | lr: 8.53e-06
  step 22750/30026 | epoch 2/2 | loss: 0.9046 | avg: 0.9259 | lr: 8.42e-06
  step 22800/30026 | epoch 2/2 | loss: 0.4294 | avg: 0.9258 | lr: 8.32e-06
  step 22850/30026 | epoch 2/2 | loss: 1.0201 | avg: 0.9259 | lr: 8.21e-06
  step 22900/30026 | epoch 2/2 | loss: 1.2375 | avg: 0.9262 | lr: 8.10e-06
  step 22950/30026 | epoch 2/2 | loss: 0.9399 | avg: 0.9264 | lr: 7.99e-06
  step 23000/30026 | epoch 2/2 | loss: 0.6055 | avg: 0.9263 | lr: 7.89e-06
  step 23050/30026 | epoch 2/2 | loss: 0.7879 | avg: 0.9262 | lr: 7.78e-06
  step 23100/30026 | epoch 2/2 | loss: 0.4350 | avg: 0.9264 | lr: 7.68e-06
  step 23150/30026 | epoch 2/2 | loss: 1.1224 | avg: 0.9260 | lr: 7.57e-06
  step 23200/30026 | epoch 2/2 | loss: 1.0725 | avg: 0.9260 | lr: 7.47e-06
  step 23250/30026 | epoch 2/2 | loss: 0.7975 | avg: 0.9256 | lr: 7.37e-06
  step 23300/30026 | epoch 2/2 | loss: 1.0408 | avg: 0.9255 | lr: 7.26e-06
  step 23350/30026 | epoch 2/2 | loss: 1.0280 | avg: 0.9254 | lr: 7.16e-06
  step 23400/30026 | epoch 2/2 | loss: 0.7724 | avg: 0.9254 | lr: 7.06e-06
  step 23450/30026 | epoch 2/2 | loss: 0.7940 | avg: 0.9254 | lr: 6.96e-06
  step 23500/30026 | epoch 2/2 | loss: 1.1304 | avg: 0.9256 | lr: 6.86e-06
  step 23550/30026 | epoch 2/2 | loss: 1.2031 | avg: 0.9253 | lr: 6.76e-06
  step 23600/30026 | epoch 2/2 | loss: 1.1757 | avg: 0.9252 | lr: 6.66e-06
  step 23650/30026 | epoch 2/2 | loss: 0.6847 | avg: 0.9252 | lr: 6.56e-06
  step 23700/30026 | epoch 2/2 | loss: 1.1300 | avg: 0.9251 | lr: 6.46e-06
  step 23750/30026 | epoch 2/2 | loss: 1.0938 | avg: 0.9253 | lr: 6.37e-06
  step 23800/30026 | epoch 2/2 | loss: 0.9435 | avg: 0.9247 | lr: 6.27e-06
  step 23850/30026 | epoch 2/2 | loss: 0.2078 | avg: 0.9246 | lr: 6.17e-06
  step 23900/30026 | epoch 2/2 | loss: 1.1787 | avg: 0.9248 | lr: 6.08e-06
  step 23950/30026 | epoch 2/2 | loss: 0.8544 | avg: 0.9248 | lr: 5.98e-06
  step 24000/30026 | epoch 2/2 | loss: 0.7012 | avg: 0.9247 | lr: 5.89e-06
  step 24050/30026 | epoch 2/2 | loss: 1.1992 | avg: 0.9252 | lr: 5.80e-06
  step 24100/30026 | epoch 2/2 | loss: 0.8800 | avg: 0.9251 | lr: 5.70e-06
  step 24150/30026 | epoch 2/2 | loss: 1.3229 | avg: 0.9252 | lr: 5.61e-06
  step 24200/30026 | epoch 2/2 | loss: 0.8762 | avg: 0.9255 | lr: 5.52e-06
  step 24250/30026 | epoch 2/2 | loss: 0.9299 | avg: 0.9255 | lr: 5.43e-06
  step 24300/30026 | epoch 2/2 | loss: 0.5293 | avg: 0.9251 | lr: 5.34e-06
  step 24350/30026 | epoch 2/2 | loss: 0.9604 | avg: 0.9253 | lr: 5.25e-06
  step 24400/30026 | epoch 2/2 | loss: 1.0014 | avg: 0.9252 | lr: 5.16e-06
  step 24450/30026 | epoch 2/2 | loss: 0.9013 | avg: 0.9251 | lr: 5.07e-06
  step 24500/30026 | epoch 2/2 | loss: 0.9909 | avg: 0.9251 | lr: 4.99e-06
  step 24550/30026 | epoch 2/2 | loss: 1.2457 | avg: 0.9252 | lr: 4.90e-06
  step 24600/30026 | epoch 2/2 | loss: 0.5989 | avg: 0.9251 | lr: 4.81e-06
  step 24650/30026 | epoch 2/2 | loss: 1.2988 | avg: 0.9251 | lr: 4.73e-06
  step 24700/30026 | epoch 2/2 | loss: 1.1070 | avg: 0.9253 | lr: 4.64e-06
  step 24750/30026 | epoch 2/2 | loss: 1.2136 | avg: 0.9252 | lr: 4.56e-06
  step 24800/30026 | epoch 2/2 | loss: 0.8918 | avg: 0.9253 | lr: 4.48e-06
  step 24850/30026 | epoch 2/2 | loss: 1.1855 | avg: 0.9253 | lr: 4.39e-06
  step 24900/30026 | epoch 2/2 | loss: 0.7226 | avg: 0.9253 | lr: 4.31e-06
  step 24950/30026 | epoch 2/2 | loss: 1.1751 | avg: 0.9253 | lr: 4.23e-06
  step 25000/30026 | epoch 2/2 | loss: 0.7026 | avg: 0.9252 | lr: 4.15e-06
2026-02-02 20:37:33,139 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_025000.pt
2026-02-02 20:37:33,139 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_025000.json
2026-02-02 20:37:35,169 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_025000_rank0.pt
  step 25050/30026 | epoch 2/2 | loss: 0.8765 | avg: 0.9252 | lr: 4.07e-06
  step 25100/30026 | epoch 2/2 | loss: 0.8339 | avg: 0.9251 | lr: 3.99e-06
  step 25150/30026 | epoch 2/2 | loss: 0.8049 | avg: 0.9253 | lr: 3.91e-06
  step 25200/30026 | epoch 2/2 | loss: 1.2704 | avg: 0.9251 | lr: 3.83e-06
  step 25250/30026 | epoch 2/2 | loss: 1.0472 | avg: 0.9251 | lr: 3.76e-06
  step 25300/30026 | epoch 2/2 | loss: 0.7994 | avg: 0.9249 | lr: 3.68e-06
  step 25350/30026 | epoch 2/2 | loss: 1.1756 | avg: 0.9250 | lr: 3.61e-06
  step 25400/30026 | epoch 2/2 | loss: 0.9110 | avg: 0.9251 | lr: 3.53e-06
  step 25450/30026 | epoch 2/2 | loss: 1.0947 | avg: 0.9251 | lr: 3.46e-06
  step 25500/30026 | epoch 2/2 | loss: 1.3699 | avg: 0.9250 | lr: 3.38e-06
  step 25550/30026 | epoch 2/2 | loss: 0.8990 | avg: 0.9254 | lr: 3.31e-06
  step 25600/30026 | epoch 2/2 | loss: 1.0677 | avg: 0.9253 | lr: 3.24e-06
  step 25650/30026 | epoch 2/2 | loss: 0.7118 | avg: 0.9253 | lr: 3.17e-06
  step 25700/30026 | epoch 2/2 | loss: 1.0801 | avg: 0.9254 | lr: 3.10e-06
  step 25750/30026 | epoch 2/2 | loss: 0.9573 | avg: 0.9255 | lr: 3.03e-06
  step 25800/30026 | epoch 2/2 | loss: 0.9423 | avg: 0.9256 | lr: 2.96e-06
  step 25850/30026 | epoch 2/2 | loss: 0.7393 | avg: 0.9256 | lr: 2.89e-06
  step 25900/30026 | epoch 2/2 | loss: 0.9181 | avg: 0.9255 | lr: 2.82e-06
  step 25950/30026 | epoch 2/2 | loss: 0.4363 | avg: 0.9255 | lr: 2.76e-06
  step 26000/30026 | epoch 2/2 | loss: 0.6693 | avg: 0.9256 | lr: 2.69e-06
  step 26050/30026 | epoch 2/2 | loss: 1.1799 | avg: 0.9257 | lr: 2.62e-06
  step 26100/30026 | epoch 2/2 | loss: 0.7696 | avg: 0.9259 | lr: 2.56e-06
  step 26150/30026 | epoch 2/2 | loss: 0.7155 | avg: 0.9258 | lr: 2.50e-06
  step 26200/30026 | epoch 2/2 | loss: 0.8683 | avg: 0.9258 | lr: 2.43e-06
  step 26250/30026 | epoch 2/2 | loss: 0.7410 | avg: 0.9257 | lr: 2.37e-06
  step 26300/30026 | epoch 2/2 | loss: 0.8839 | avg: 0.9256 | lr: 2.31e-06
  step 26350/30026 | epoch 2/2 | loss: 0.8336 | avg: 0.9259 | lr: 2.25e-06
  step 26400/30026 | epoch 2/2 | loss: 0.5752 | avg: 0.9259 | lr: 2.19e-06
  step 26450/30026 | epoch 2/2 | loss: 0.5385 | avg: 0.9260 | lr: 2.13e-06
  step 26500/30026 | epoch 2/2 | loss: 0.9795 | avg: 0.9261 | lr: 2.07e-06
  step 26550/30026 | epoch 2/2 | loss: 1.1828 | avg: 0.9260 | lr: 2.01e-06
  step 26600/30026 | epoch 2/2 | loss: 0.6110 | avg: 0.9260 | lr: 1.96e-06
  step 26650/30026 | epoch 2/2 | loss: 1.1310 | avg: 0.9262 | lr: 1.90e-06
  step 26700/30026 | epoch 2/2 | loss: 0.5042 | avg: 0.9262 | lr: 1.85e-06
  step 26750/30026 | epoch 2/2 | loss: 0.8118 | avg: 0.9261 | lr: 1.79e-06
  step 26800/30026 | epoch 2/2 | loss: 0.4884 | avg: 0.9264 | lr: 1.74e-06
  step 26850/30026 | epoch 2/2 | loss: 0.5284 | avg: 0.9263 | lr: 1.69e-06
  step 26900/30026 | epoch 2/2 | loss: 0.8650 | avg: 0.9262 | lr: 1.63e-06
  step 26950/30026 | epoch 2/2 | loss: 1.1131 | avg: 0.9262 | lr: 1.58e-06
  step 27000/30026 | epoch 2/2 | loss: 0.2540 | avg: 0.9261 | lr: 1.53e-06
  step 27050/30026 | epoch 2/2 | loss: 0.9822 | avg: 0.9262 | lr: 1.48e-06
  step 27100/30026 | epoch 2/2 | loss: 1.0377 | avg: 0.9262 | lr: 1.43e-06
  step 27150/30026 | epoch 2/2 | loss: 0.9272 | avg: 0.9262 | lr: 1.39e-06
  step 27200/30026 | epoch 2/2 | loss: 0.9225 | avg: 0.9262 | lr: 1.34e-06
  step 27250/30026 | epoch 2/2 | loss: 2.1236 | avg: 0.9265 | lr: 1.29e-06
  step 27300/30026 | epoch 2/2 | loss: 0.6646 | avg: 0.9264 | lr: 1.25e-06
  step 27350/30026 | epoch 2/2 | loss: 0.5878 | avg: 0.9264 | lr: 1.20e-06
  step 27400/30026 | epoch 2/2 | loss: 0.9680 | avg: 0.9262 | lr: 1.16e-06
  step 27450/30026 | epoch 2/2 | loss: 0.5945 | avg: 0.9263 | lr: 1.11e-06
  step 27500/30026 | epoch 2/2 | loss: 0.8937 | avg: 0.9262 | lr: 1.07e-06
  step 27550/30026 | epoch 2/2 | loss: 1.0007 | avg: 0.9263 | lr: 1.03e-06
  step 27600/30026 | epoch 2/2 | loss: 1.0413 | avg: 0.9264 | lr: 9.88e-07
  step 27650/30026 | epoch 2/2 | loss: 0.7760 | avg: 0.9266 | lr: 9.48e-07
  step 27700/30026 | epoch 2/2 | loss: 0.6470 | avg: 0.9266 | lr: 9.09e-07
  step 27750/30026 | epoch 2/2 | loss: 0.6744 | avg: 0.9268 | lr: 8.71e-07
  step 27800/30026 | epoch 2/2 | loss: 0.7983 | avg: 0.9269 | lr: 8.33e-07
  step 27850/30026 | epoch 2/2 | loss: 1.1722 | avg: 0.9270 | lr: 7.96e-07
  step 27900/30026 | epoch 2/2 | loss: 0.1591 | avg: 0.9268 | lr: 7.60e-07
  step 27950/30026 | epoch 2/2 | loss: 0.8002 | avg: 0.9268 | lr: 7.25e-07
  step 28000/30026 | epoch 2/2 | loss: 0.5311 | avg: 0.9268 | lr: 6.91e-07
  step 28050/30026 | epoch 2/2 | loss: 1.0158 | avg: 0.9271 | lr: 6.57e-07
  step 28100/30026 | epoch 2/2 | loss: 0.2660 | avg: 0.9268 | lr: 6.25e-07
  step 28150/30026 | epoch 2/2 | loss: 0.8220 | avg: 0.9269 | lr: 5.93e-07
  step 28200/30026 | epoch 2/2 | loss: 0.7385 | avg: 0.9268 | lr: 5.62e-07
  step 28250/30026 | epoch 2/2 | loss: 0.2496 | avg: 0.9267 | lr: 5.32e-07
  step 28300/30026 | epoch 2/2 | loss: 0.8842 | avg: 0.9269 | lr: 5.02e-07
  step 28350/30026 | epoch 2/2 | loss: 0.4215 | avg: 0.9268 | lr: 4.74e-07
  step 28400/30026 | epoch 2/2 | loss: 0.5289 | avg: 0.9268 | lr: 4.46e-07
  step 28450/30026 | epoch 2/2 | loss: 1.2304 | avg: 0.9268 | lr: 4.19e-07
  step 28500/30026 | epoch 2/2 | loss: 1.1041 | avg: 0.9268 | lr: 3.93e-07
  step 28550/30026 | epoch 2/2 | loss: 1.0177 | avg: 0.9267 | lr: 3.68e-07
  step 28600/30026 | epoch 2/2 | loss: 0.7601 | avg: 0.9267 | lr: 3.43e-07
  step 28650/30026 | epoch 2/2 | loss: 0.7314 | avg: 0.9266 | lr: 3.20e-07
  step 28700/30026 | epoch 2/2 | loss: 0.8361 | avg: 0.9266 | lr: 2.97e-07
  step 28750/30026 | epoch 2/2 | loss: 0.8135 | avg: 0.9266 | lr: 2.75e-07
  step 28800/30026 | epoch 2/2 | loss: 1.1621 | avg: 0.9267 | lr: 2.54e-07
  step 28850/30026 | epoch 2/2 | loss: 0.9414 | avg: 0.9269 | lr: 2.34e-07
  step 28900/30026 | epoch 2/2 | loss: 0.6052 | avg: 0.9268 | lr: 2.14e-07
  step 28950/30026 | epoch 2/2 | loss: 0.9803 | avg: 0.9267 | lr: 1.96e-07
  step 29000/30026 | epoch 2/2 | loss: 0.4668 | avg: 0.9268 | lr: 1.78e-07
  step 29050/30026 | epoch 2/2 | loss: 1.6140 | avg: 0.9269 | lr: 1.61e-07
  step 29100/30026 | epoch 2/2 | loss: 0.8896 | avg: 0.9268 | lr: 1.45e-07
  step 29150/30026 | epoch 2/2 | loss: 0.4320 | avg: 0.9268 | lr: 1.30e-07
  step 29200/30026 | epoch 2/2 | loss: 0.8064 | avg: 0.9267 | lr: 1.15e-07
  step 29250/30026 | epoch 2/2 | loss: 1.0764 | avg: 0.9268 | lr: 1.02e-07
  step 29300/30026 | epoch 2/2 | loss: 1.1172 | avg: 0.9269 | lr: 8.92e-08
  step 29350/30026 | epoch 2/2 | loss: 0.8237 | avg: 0.9268 | lr: 7.74e-08
  step 29400/30026 | epoch 2/2 | loss: 1.0210 | avg: 0.9269 | lr: 6.64e-08
  step 29450/30026 | epoch 2/2 | loss: 0.9340 | avg: 0.9270 | lr: 5.62e-08
  step 29500/30026 | epoch 2/2 | loss: 0.4935 | avg: 0.9270 | lr: 4.69e-08
  step 29550/30026 | epoch 2/2 | loss: 0.7478 | avg: 0.9271 | lr: 3.84e-08
  step 29600/30026 | epoch 2/2 | loss: 0.7797 | avg: 0.9270 | lr: 3.08e-08
  step 29650/30026 | epoch 2/2 | loss: 1.2233 | avg: 0.9271 | lr: 2.40e-08
  step 29700/30026 | epoch 2/2 | loss: 0.8826 | avg: 0.9271 | lr: 1.81e-08
  step 29750/30026 | epoch 2/2 | loss: 1.0322 | avg: 0.9273 | lr: 1.30e-08
  step 29800/30026 | epoch 2/2 | loss: 0.6920 | avg: 0.9273 | lr: 8.71e-09
  step 29850/30026 | epoch 2/2 | loss: 0.8102 | avg: 0.9275 | lr: 5.29e-09
  step 29900/30026 | epoch 2/2 | loss: 0.5921 | avg: 0.9275 | lr: 2.73e-09
  step 29950/30026 | epoch 2/2 | loss: 0.7810 | avg: 0.9274 | lr: 1.00e-09
  step 30000/30026 | epoch 2/2 | loss: 1.0122 | avg: 0.9276 | lr: 1.24e-10
2026-02-02 21:36:49,734 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_030000.pt
2026-02-02 21:36:49,735 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_030000.json
2026-02-02 21:36:51,766 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_030000_rank0.pt
Epoch 2/2 done in 10564.9s | avg loss: 0.9277 | tokens: 29,373,984
2026-02-02 21:37:11,884 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved model parameters to: /home/dave/.cache/nanochat/sft_checkpoints/d16/model_030026.pt
2026-02-02 21:37:11,885 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved metadata to: /home/dave/.cache/nanochat/sft_checkpoints/d16/meta_030026.json
2026-02-02 21:37:13,848 - nanochat.checkpoint_manager - [32m[1mINFO[0m - Saved optimizer state to: /home/dave/.cache/nanochat/sft_checkpoints/d16/optim_030026_rank0.pt
Training complete. Checkpoint saved to /home/dave/.cache/nanochat/sft_checkpoints/d16
