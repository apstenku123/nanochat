"""
Extract and normalize C++ files from downloaded source repos.
Produces a single JSONL file suitable for tokenizer training.

Usage:
    python -m scripts.data.extract_cpp [--raw_dir data/cpp_raw] [--output data/cpp_clean.jsonl] [--max_gb 5]
"""
import os
import re
import json
import argparse
from pathlib import Path

CPP_EXTENSIONS = {'.cpp', '.cc', '.cxx', '.c', '.h', '.hpp', '.hxx', '.cu', '.cuh'}

# Skip these directories (build artifacts, not source code)
# NOTE: We intentionally include third_party/vendor/external because they contain
# real C++ source code useful for tokenizer training.
SKIP_DIRS = {
    '.git', 'build', 'cmake-build',
    'node_modules', '.deps',
    'testdata', 'benchmarks/data',
}

# Auto-generated file markers
GENERATED_MARKERS = [
    'DO NOT EDIT', 'Generated by', 'AUTO-GENERATED', 'GENERATED FILE',
    'This file was generated', 'machine generated', 'auto-generated',
    '@generated', 'Code generated by',
]

def is_generated(content: str, max_check: int = 500) -> bool:
    header = content[:max_check].upper()
    return any(m.upper() in header for m in GENERATED_MARKERS)

def strip_non_ascii_lines(code: str) -> str:
    """Remove lines containing non-ASCII characters.

    After comment translation, remaining non-ASCII is in string literals,
    identifiers, or test data (CJK text, box-drawing chars, Unicode test
    vectors).  These don't teach C++ reasoning so we drop the entire line
    to keep the corpus ASCII-clean.

    Lines that are part of a multi-line string literal or block comment may
    leave orphaned delimiters — the quality filter downstream will catch files
    that become too small or malformed.
    """
    lines = code.split('\n')
    cleaned = []
    for line in lines:
        try:
            line.encode('ascii')
            cleaned.append(line)
        except UnicodeEncodeError:
            # Drop the line — it contains non-ASCII characters
            continue
    return '\n'.join(cleaned)


def normalize_cpp(code: str) -> str:
    """Normalize C++ code: preserve indentation, compress blank lines, strip trailing whitespace.

    We keep indentation because it carries structural information and contributes
    meaningful tokens for a code model.  Tabs are converted to 4 spaces for
    consistency.  Consecutive blank lines are collapsed to one.
    """
    lines = code.split('\n')
    result = []
    prev_blank = False
    for line in lines:
        line = line.replace('\t', '    ').rstrip()
        if not line:
            if not prev_blank:
                result.append('')
                prev_blank = True
            continue
        prev_blank = False
        result.append(line)
    return '\n'.join(result)

def strip_license_header(code: str) -> str:
    """Remove license/copyright block comment at the top of file."""
    # Match /* ... */ block at very start (possibly after blank lines)
    m = re.match(r'^\s*/\*.*?\*/\s*\n?', code, re.DOTALL)
    if m:
        block = m.group()
        # Only strip if it looks like license (contains copyright/license keywords)
        block_lower = block.lower()
        if any(kw in block_lower for kw in ['copyright', 'license', 'permission is hereby granted',
                                              'redistribution', 'all rights reserved', 'spdx']):
            return code[m.end():]
    # Also match // comment block at top
    lines = code.split('\n')
    i = 0
    while i < len(lines) and i < 30:
        stripped = lines[i].strip()
        if stripped.startswith('//'):
            i += 1
        elif stripped == '':
            i += 1
        else:
            break
    if i > 3:
        header = '\n'.join(lines[:i]).lower()
        if any(kw in header for kw in ['copyright', 'license', 'permission is hereby granted']):
            return '\n'.join(lines[i:])
    return code

def quality_filter(content: str) -> bool:
    """Return True if file passes quality checks."""
    # Size checks
    if len(content) < 100:
        return False
    if len(content) > 2_000_000:
        return False
    # Line length check
    lines = content.split('\n')
    if any(len(l) > 2000 for l in lines[:100]):
        return False
    # Unique lines ratio
    if len(lines) > 10:
        unique = len(set(lines))
        if unique / len(lines) < 0.3:
            return False
    # Not auto-generated
    if is_generated(content):
        return False
    return True

def should_skip_dir(dirpath: str) -> bool:
    parts = Path(dirpath).parts
    return any(p in SKIP_DIRS for p in parts)

def extract_files(raw_dir: str, output_path: str, max_bytes: int):
    """Walk raw_dir, extract C++ files, normalize, write JSONL."""
    total_bytes = 0
    total_files = 0
    skipped = 0

    with open(output_path, 'w') as out:
        for root, dirs, files in os.walk(raw_dir):
            # Prune skip dirs
            dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.startswith('.')]

            if should_skip_dir(root):
                continue

            for fname in files:
                ext = os.path.splitext(fname)[1].lower()
                if ext not in CPP_EXTENSIONS:
                    continue

                fpath = os.path.join(root, fname)
                try:
                    with open(fpath, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                except (OSError, UnicodeDecodeError):
                    skipped += 1
                    continue

                if not quality_filter(content):
                    skipped += 1
                    continue

                # Process
                content = strip_license_header(content)
                content = strip_non_ascii_lines(content)
                content = normalize_cpp(content)

                if len(content) < 50:
                    skipped += 1
                    continue

                # Relative path for metadata
                rel_path = os.path.relpath(fpath, raw_dir)
                record = {"text": content, "path": rel_path}
                line = json.dumps(record, ensure_ascii=True)
                out.write(line + '\n')

                total_bytes += len(content)
                total_files += 1

                if total_files % 10000 == 0:
                    print(f"  {total_files:,} files, {total_bytes/1e9:.2f} GB")

                if total_bytes >= max_bytes:
                    print(f"Reached {max_bytes/1e9:.1f} GB limit")
                    break
            else:
                continue
            break

    print(f"\nDone: {total_files:,} files, {total_bytes/1e9:.2f} GB text")
    print(f"Skipped: {skipped:,} files")
    print(f"Output: {output_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--raw_dir', default='data/cpp_raw')
    parser.add_argument('--output', default='data/cpp_clean.jsonl')
    parser.add_argument('--max_gb', type=float, default=5.0)
    args = parser.parse_args()

    raw_dir = os.path.join(os.path.dirname(__file__), '..', '..', args.raw_dir)
    raw_dir = os.path.abspath(raw_dir)
    output = os.path.join(os.path.dirname(__file__), '..', '..', args.output)
    output = os.path.abspath(output)
    os.makedirs(os.path.dirname(output), exist_ok=True)

    print(f"Extracting C++ from: {raw_dir}")
    print(f"Output: {output}")
    print(f"Max size: {args.max_gb} GB")
    extract_files(raw_dir, output, int(args.max_gb * 1e9))
