"""
Extract, clean, deduplicate, and pack C/C++ code from Microsoft source archives.

Downloads are 7z archives from gs://ms_src containing Windows SDK, NT source,
Xbox, Windows CE, etc.

Pipeline:
  1. Extract C/C++ files from each 7z archive
  2. Normalize (tabs→spaces, strip license headers, collapse blanks)
  3. Deduplicate (exact hash + near-duplicate by first 1KB)
  4. Output JSONL with {"text": "..."} format (same as cpp_clean.jsonl)
  5. Convert to parquet shards for training

Usage:
    python -m scripts.data.process_ms_src --input_dir /home/dave/Downloads/ms_src --output data/ms_src.jsonl
    python -m scripts.data.process_ms_src --input_dir /home/dave/Downloads/ms_src --output data/ms_src.jsonl --to_parquet
"""

import argparse
import hashlib
import json
import os
import re
import subprocess
import sys
import tempfile
from collections import Counter
from pathlib import Path

CPP_EXTENSIONS = {'.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.hxx', '.inl', '.ipp'}

# Directories to skip inside archives
SKIP_DIRS = {
    '.git', 'build', 'cmake-build', 'node_modules', '.deps',
    '__pycache__', '.svn', 'Debug', 'Release', 'x64', 'x86',
    'obj', 'bin',
}

# Auto-generated file markers
GENERATED_MARKERS = [
    'DO NOT EDIT', 'Generated by', 'AUTO-GENERATED', 'GENERATED FILE',
    'This file was generated', 'machine generated', 'auto-generated',
    '@generated', 'Code generated by', 'AUTOGENERATED',
    '// This file is automatically generated',
    '/* This file is automatically generated',
]

MIN_FILE_BYTES = 100       # Skip tiny files
MAX_FILE_BYTES = 1_000_000  # Skip files > 1MB (likely generated/data)
MIN_CODE_LINES = 5          # Must have at least 5 non-blank lines


def is_generated(content: str, max_check: int = 500) -> bool:
    header = content[:max_check].upper()
    return any(m.upper() in header for m in GENERATED_MARKERS)


def is_binary_content(content: str, check_len: int = 1024) -> bool:
    """Check if content looks binary (lots of null bytes or non-printable chars)."""
    sample = content[:check_len]
    if '\x00' in sample:
        return True
    non_printable = sum(1 for c in sample if ord(c) < 32 and c not in '\n\r\t')
    return non_printable > len(sample) * 0.1


def normalize_cpp(code: str) -> str:
    """Normalize C++ code: tabs→4 spaces, collapse blank lines, strip trailing ws."""
    lines = code.split('\n')
    result = []
    prev_blank = False
    for line in lines:
        line = line.replace('\t', '    ').rstrip()
        if not line:
            if not prev_blank:
                result.append('')
                prev_blank = True
            continue
        prev_blank = False
        result.append(line)
    return '\n'.join(result).strip()


def strip_license_header(code: str) -> str:
    """Remove license/copyright block comment at top of file."""
    m = re.match(r'^\s*/\*.*?\*/\s*\n?', code, re.DOTALL)
    if m:
        block_lower = m.group().lower()
        if any(kw in block_lower for kw in ['copyright', 'license', 'permission is hereby granted',
                                              'redistribution', 'all rights reserved', 'spdx']):
            code = code[m.end():]
    # Also strip // comment block at top
    lines = code.split('\n')
    i = 0
    while i < len(lines) and i < 30:
        stripped = lines[i].strip()
        if stripped.startswith('//'):
            i += 1
        elif stripped == '':
            i += 1
        else:
            break
    if i > 3:
        header = '\n'.join(lines[:i]).lower()
        if any(kw in header for kw in ['copyright', 'license', 'all rights reserved']):
            code = '\n'.join(lines[i:])
    return code


def count_code_lines(code: str) -> int:
    """Count non-blank, non-comment lines."""
    count = 0
    for line in code.split('\n'):
        stripped = line.strip()
        if stripped and not stripped.startswith('//') and not stripped.startswith('/*') and stripped != '*/' and not stripped.startswith('*'):
            count += 1
    return count


def should_skip_path(path: str) -> bool:
    """Check if a file path should be skipped based on directory names."""
    parts = Path(path).parts
    return any(p in SKIP_DIRS for p in parts)


def extract_7z(archive_path: str, output_dir: str) -> bool:
    """Extract a 7z archive. Returns True on success."""
    try:
        result = subprocess.run(
            ['7z', 'x', '-y', f'-o{output_dir}', archive_path],
            capture_output=True, text=True, timeout=600,
        )
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError) as e:
        print(f"  ERROR extracting {archive_path}: {e}")
        return False


def process_archive(archive_path: str, extract_base: str, seen_hashes: set,
                    near_dup_hashes: set, stats: Counter) -> list[dict]:
    """Extract and process a single archive, returning list of {"text": ...} dicts."""
    archive_name = os.path.basename(archive_path).replace('.7z', '')
    extract_dir = os.path.join(extract_base, archive_name)
    os.makedirs(extract_dir, exist_ok=True)

    print(f"\n--- Extracting {archive_name} ({os.path.getsize(archive_path) / 1e6:.0f} MB) ---")
    if not extract_7z(archive_path, extract_dir):
        stats['archives_failed'] += 1
        return []
    stats['archives_extracted'] += 1

    results = []
    file_count = 0

    for root, dirs, files in os.walk(extract_dir):
        # Prune skip dirs
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in CPP_EXTENSIONS:
                continue

            fpath = os.path.join(root, fname)
            rel_path = os.path.relpath(fpath, extract_dir)

            if should_skip_path(rel_path):
                stats['skipped_dir'] += 1
                continue

            file_count += 1
            stats['files_found'] += 1

            # Size check
            try:
                fsize = os.path.getsize(fpath)
            except OSError:
                continue
            if fsize < MIN_FILE_BYTES:
                stats['too_small'] += 1
                continue
            if fsize > MAX_FILE_BYTES:
                stats['too_large'] += 1
                continue

            # Read
            try:
                with open(fpath, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
            except Exception:
                stats['read_error'] += 1
                continue

            # Binary check
            if is_binary_content(content):
                stats['binary'] += 1
                continue

            # Generated check
            if is_generated(content):
                stats['generated'] += 1
                continue

            # Normalize
            content = strip_license_header(content)
            content = normalize_cpp(content)

            # Code line count
            if count_code_lines(content) < MIN_CODE_LINES:
                stats['too_few_lines'] += 1
                continue

            # Exact dedup (full content hash)
            content_hash = hashlib.md5(content.encode()).hexdigest()
            if content_hash in seen_hashes:
                stats['exact_dup'] += 1
                continue
            seen_hashes.add(content_hash)

            # Near-dedup (first 1KB hash — catches files with only minor diffs)
            near_hash = hashlib.md5(content[:1024].encode()).hexdigest()
            if near_hash in near_dup_hashes:
                stats['near_dup'] += 1
                continue
            near_dup_hashes.add(near_hash)

            results.append({"text": content})
            stats['kept'] += 1

    print(f"  {archive_name}: {file_count} C/C++ files → {len(results)} kept")

    # Clean up extracted files to save disk
    subprocess.run(['rm', '-rf', extract_dir], capture_output=True)

    return results


def to_parquet(jsonl_path: str, output_dir: str, rows_per_file: int = 50000):
    """Convert JSONL to parquet shards."""
    import random
    import pyarrow as pa
    import pyarrow.parquet as pq

    print(f"\nConverting {jsonl_path} to parquet shards...")
    texts = []
    with open(jsonl_path) as f:
        for line in f:
            if line.strip():
                texts.append(json.loads(line)['text'])

    print(f"Total documents: {len(texts):,}")
    random.seed(42)
    random.shuffle(texts)

    # 99% train, 1% val
    val_count = max(1, int(len(texts) * 0.01))
    train_texts = texts[:-val_count]
    val_texts = texts[-val_count:]

    os.makedirs(output_dir, exist_ok=True)
    shard_idx = 0
    for start in range(0, len(train_texts), rows_per_file):
        batch = train_texts[start:start + rows_per_file]
        table = pa.table({'text': batch})
        path = os.path.join(output_dir, f"shard_{shard_idx:05d}.parquet")
        pq.write_table(table, path, row_group_size=1024)
        shard_idx += 1

    val_table = pa.table({'text': val_texts})
    val_path = os.path.join(output_dir, f"shard_{shard_idx:05d}.parquet")
    pq.write_table(val_table, val_path, row_group_size=1024)

    print(f"Written {shard_idx} train shards + 1 val shard to {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Process Microsoft source archives for C++ training")
    parser.add_argument("--input_dir", type=str, default="/home/dave/Downloads/ms_src",
                        help="Directory containing .7z archives")
    parser.add_argument("--output", type=str, default="data/ms_src.jsonl",
                        help="Output JSONL path")
    parser.add_argument("--extract_dir", type=str, default="/tmp/ms_src_extract",
                        help="Temp directory for extraction (cleaned up per archive)")
    parser.add_argument("--to_parquet", action="store_true",
                        help="Also convert to parquet shards")
    parser.add_argument("--parquet_dir", type=str, default="",
                        help="Parquet output dir (default: data/ms_src_parquet/)")
    args = parser.parse_args()

    # Find all archives
    archives = sorted(
        os.path.join(args.input_dir, f)
        for f in os.listdir(args.input_dir)
        if f.endswith('.7z')
    )
    print(f"Found {len(archives)} archives in {args.input_dir}")
    total_size = sum(os.path.getsize(a) for a in archives)
    print(f"Total compressed size: {total_size / 1e9:.1f} GB")

    # Process each archive
    os.makedirs(args.extract_dir, exist_ok=True)
    seen_hashes = set()
    near_dup_hashes = set()
    stats = Counter()
    all_results = []

    for archive in archives:
        results = process_archive(archive, args.extract_dir, seen_hashes, near_dup_hashes, stats)
        all_results.extend(results)
        print(f"  Running total: {len(all_results):,} files kept")

    # Write JSONL
    os.makedirs(os.path.dirname(args.output) or '.', exist_ok=True)
    with open(args.output, 'w') as f:
        for item in all_results:
            f.write(json.dumps(item) + '\n')

    output_size = os.path.getsize(args.output) / 1e9
    print(f"\n{'='*60}")
    print(f"RESULTS:")
    print(f"  Archives: {stats['archives_extracted']}/{len(archives)} extracted ({stats['archives_failed']} failed)")
    print(f"  C/C++ files found: {stats['files_found']:,}")
    print(f"  Kept: {stats['kept']:,}")
    print(f"  Filtered:")
    print(f"    Too small: {stats['too_small']:,}")
    print(f"    Too large: {stats['too_large']:,}")
    print(f"    Binary: {stats['binary']:,}")
    print(f"    Generated: {stats['generated']:,}")
    print(f"    Too few lines: {stats['too_few_lines']:,}")
    print(f"    Exact duplicates: {stats['exact_dup']:,}")
    print(f"    Near duplicates: {stats['near_dup']:,}")
    print(f"    Skip dir: {stats['skipped_dir']:,}")
    print(f"    Read error: {stats['read_error']:,}")
    print(f"  Output: {args.output} ({output_size:.2f} GB)")
    print(f"{'='*60}")

    # Convert to parquet if requested
    if args.to_parquet:
        parquet_dir = args.parquet_dir or os.path.splitext(args.output)[0] + '_parquet'
        to_parquet(args.output, parquet_dir)


if __name__ == "__main__":
    main()
